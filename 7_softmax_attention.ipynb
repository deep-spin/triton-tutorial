{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a5e60-b6e7-459a-b2ec-6ba79e2d1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_triton import setup_triton\n",
    "\n",
    "# TRITON_INTERPRET=1 uses a python interpreter instead of running on the GPU. \n",
    "# This menas that uou can insert Python breakpoints to debug your kernel code! \n",
    "setup_triton(use_interpreter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857c39a-c86f-4acf-8162-eaf383c3f3c4",
   "metadata": {},
   "source": [
    "# Triton Puzzle 7: Softmax Attention\n",
    "\n",
    "Welcome to the last Triton puzzle! Attention is the cornerstone of modern transformers. This puzzle teaches you how to implement the (forward pass of the) scaled dot-product attention mechanism efficiently.\n",
    "\n",
    "### What you'll learn:\n",
    "- **Multi-stage computation** with multiple matrix operations\n",
    "- **2D softmax** across sequence dimension\n",
    "- **Causal masking** for autoregressive models\n",
    "- **Memory-efficient attention** for long sequences\n",
    "- **Numerical stability** with proper scaling\n",
    "- Introduction to the _main_ ideas behind **Flash Attention**\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "Scaled dot-product attention computes:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(Q K^\\top / \\sqrt{d} \\right) V\n",
    "$$\n",
    "\n",
    "Where: $Q, K, V \\in \\mathbb{R}^{B \\times H \\times N \\times d}$, with B = batch size, H = number of heads, N = sequence length, d = head size.\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "Standard attention has $O(N^2)$ memory complexity:\n",
    "1. Compute scores: $S = Q K^\\top$, a (B x H x N x N) tensor\n",
    "2. Apply softmax: $P = \\text{softmax}(S)$, still (B x H x N x N)\n",
    "3. Compute output: $O = PV$, with (B x H x N x d_v)\n",
    "\n",
    "For long sequences, the N x N attention matrix becomes prohibitive!\n",
    "\n",
    "\n",
    "### Flash Attention Insight\n",
    "\n",
    "Instead of materializing the full N x N attention matrix:\n",
    "- Process attention in blocks\n",
    "- Recompute rather than store intermediate values\n",
    "- Use online softmax for numerical stability and faster computation\n",
    "\n",
    "💡 Final Tip: Remember that you can use `TRITON_INTERPRET=1` to debug your kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de76882-1609-4697-940f-5e59c1ed22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48ca9d-ccd2-4eeb-bb8c-91710a0978f3",
   "metadata": {},
   "source": [
    "## Implementation 1: Naive PyTorch\n",
    "\n",
    "First, let's see the standard implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c9c8a-fbf0-4499-a471-e980dfb8b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_naive(Q, K, V, causal=False):\n",
    "    \"\"\"\n",
    "    Naive attention implementation.\n",
    "    Creates the full N×N attention matrix.\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: (B, H, N, d) tensors\n",
    "        causal: whether to apply causal mask\n",
    "    \"\"\"\n",
    "    B, H, N, d = Q.shape\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d ** 0.5)  # (B, H, N, N)\n",
    "    \n",
    "    # Apply causal mask if needed\n",
    "    if causal:\n",
    "        mask = torch.triu(torch.ones(N, N, device=Q.device), diagonal=1).bool()\n",
    "        scores.masked_fill_(mask[None, None, :, :], float('-inf'))\n",
    "    \n",
    "    # Apply softmax\n",
    "    attn_weights = torch.softmax(scores, dim=-1)  # (B, H, N, N)\n",
    "    \n",
    "    # Compute output\n",
    "    out = torch.matmul(attn_weights, V)  # (B, H, N, d)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab05fc-f22d-4bcc-976e-3fb4b382e092",
   "metadata": {},
   "source": [
    "## Implementation 2: PyTorch Built-in\n",
    "\n",
    "PyTorch's scaled_dot_product_attention (introduced in PyTorch 2.0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9751e0a-713e-48b0-9cde-ae0dffc60982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_pytorch(Q, K, V, causal=False):\n",
    "    \"\"\"PyTorch's built-in attention (uses Flash Attention when possible).\"\"\"\n",
    "    # Note: PyTorch expects (B, H, N, d) format\n",
    "    return torch.nn.functional.scaled_dot_product_attention(\n",
    "        Q, K, V, \n",
    "        is_causal=causal,\n",
    "        dropout_p=0.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544f516c-cb7a-434d-9de0-2cff19cca146",
   "metadata": {},
   "source": [
    "## Implementation 3: PyTorch Compiled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc0997-bf70-499a-934a-fc74f36f9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def attention_compiled(Q, K, V, causal=False):\n",
    "    \"\"\"Compiled attention.\"\"\"\n",
    "    B, H, N, d = Q.shape\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d ** 0.5)\n",
    "    if causal:\n",
    "        mask = torch.triu(torch.ones(N, N, device=Q.device), diagonal=1).bool()\n",
    "        scores.masked_fill_(mask, float('-inf'))\n",
    "    attn_weights = torch.softmax(scores, dim=-1)\n",
    "    out = torch.matmul(attn_weights, V)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f89ace-f213-435e-8f42-df385f5bf256",
   "metadata": {},
   "source": [
    "## Key Concepts for This Puzzle\n",
    "\n",
    "### 1. Block Processing Strategy\n",
    "\n",
    "Instead of computing the full attention matrix:\n",
    "```python\n",
    "# Naive: O(N^2) memory\n",
    "scores = Q @ K.T  # N×N matrix\n",
    "\n",
    "# Block-wise: O(N) memory\n",
    "for block_q in Q_blocks:\n",
    "    for block_k in K_blocks:\n",
    "        block_scores = block_q @ block_k.T  # Bq x Bk matrix\n",
    "        # Process block...\n",
    "```\n",
    "\n",
    "### 2. Online Softmax for 2D\n",
    "\n",
    "Extending our 1D online softmax to 2D:\n",
    "- Each query attends to all keys\n",
    "- Need to track max and sum for each query\n",
    "- Update statistics as we process key blocks\n",
    "\n",
    "### 3. Causal Masking\n",
    "\n",
    "For autoregressive models:\n",
    "- Query at position _i_ can only attend to positions _[1, ..., i]_\n",
    "- Implement efficiently without creating full mask matrix\n",
    "\n",
    "### 4. Memory vs Recomputation Trade-off\n",
    "\n",
    "- Standard: Store N x N attention matrix\n",
    "- Memory-efficient: Recompute blocks as needed\n",
    "- Flash Attention: Never materialize full attention matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd036c-cee3-44a8-855b-ea8b6662f71e",
   "metadata": {},
   "source": [
    "## Implementation 4: Triton Kernel (Puzzle)\n",
    "\n",
    "Now implement memory-efficient attention!\n",
    "\n",
    "We'll start with a simpler version that still materializes attention weights (but processes in blocks), then show the full Flash Attention style approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee9cd2-51bc-4a32-84de-2894a8bb45e0",
   "metadata": {},
   "source": [
    "### Version 1: Block-wise Attention (Starter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21248f61-c6f9-46af-a41f-f8941944921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def attention_kernel_v1(\n",
    "    Q_ptr, K_ptr, V_ptr, Out_ptr,\n",
    "    B, H, N, d,\n",
    "    scale,\n",
    "    stride_qb, stride_qh, stride_qn, stride_qd,\n",
    "    stride_kb, stride_kh, stride_kn, stride_kd,\n",
    "    stride_vb, stride_vh, stride_vn, stride_vd,\n",
    "    stride_ob, stride_oh, stride_on, stride_od,\n",
    "    causal: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "    BLOCK_D: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simplified attention kernel - still computes full attention per query.\n",
    "    Each program handles one (batch, head, query) combination.\n",
    "    \n",
    "    This version is easier to understand but less memory efficient.\n",
    "    \"\"\"\n",
    "    # YOUR IMPLEMENTATION GOES HERE\n",
    "    # Hints:\n",
    "    # 1. Get batch, head, and query indices from program ID\n",
    "    # 2. Load the query vector (size d)\n",
    "    # 3. Loop over all keys to compute attention scores\n",
    "    # 4. Apply softmax to get weights\n",
    "    # 5. Use weights to compute weighted sum of values\n",
    "    pass\n",
    "\n",
    "\n",
    "def attention_triton_v1(Q, K, V, causal=False):\n",
    "    \"\"\"Wrapper for Triton attention kernel.\"\"\"\n",
    "    assert Q.shape == K.shape\n",
    "    assert K.shape[-1] == V.shape[-1]\n",
    "    B, H, N, d = Q.shape\n",
    "    scale = 1.0 / (d**0.5)\n",
    "    \n",
    "    # Allocate output\n",
    "    Out = torch.empty_like(Q).contiguous()\n",
    "\n",
    "    # Ensure contiguity\n",
    "    Q = Q.contiguous()\n",
    "    K = K.contiguous()\n",
    "    V = V.contiguous()\n",
    "    \n",
    "    # Choose block sizes\n",
    "    BLOCK_N = min(triton.next_power_of_2(N), 64)\n",
    "    BLOCK_D = d  # = d for simplicity\n",
    "    \n",
    "    # Launch kernel (one program per batch, head, query)\n",
    "    grid = (B * H * N,)\n",
    "    \n",
    "    attention_kernel_v1[grid](\n",
    "        Q, K, V, Out,\n",
    "        B, H, N, d,\n",
    "        scale,\n",
    "        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),\n",
    "        K.stride(0), K.stride(1), K.stride(2), K.stride(3),\n",
    "        V.stride(0), V.stride(1), V.stride(2), V.stride(3),\n",
    "        Out.stride(0), Out.stride(1), Out.stride(2), Out.stride(3),\n",
    "        causal,\n",
    "        BLOCK_N, \n",
    "        BLOCK_D,\n",
    "    )\n",
    "    \n",
    "    return Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e67656-cbc9-43ab-b3d6-ea667b1335a2",
   "metadata": {},
   "source": [
    "### Version 2: Flash Attention Style (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab591d-be80-43b2-958a-3a2442baea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def attention_kernel_v2(\n",
    "    Q_ptr, K_ptr, V_ptr, Out_ptr,\n",
    "    B, H, N, d,\n",
    "    scale,\n",
    "    stride_qb, stride_qh, stride_qn, stride_qd,\n",
    "    stride_kb, stride_kh, stride_kn, stride_kd,\n",
    "    stride_vb, stride_vh, stride_vn, stride_vd,\n",
    "    stride_ob, stride_oh, stride_on, stride_od,\n",
    "    causal: tl.constexpr,\n",
    "    BLOCK_M: tl.constexpr,  # Block size for queries\n",
    "    BLOCK_N: tl.constexpr,  # Block size for keys/values\n",
    "    BLOCK_D: tl.constexpr,  # Block size for head dimension\n",
    "):\n",
    "    \"\"\"\n",
    "    Flash Attention style kernel - never materializes full attention.\n",
    "    Each program handles a block of queries.\n",
    "    \n",
    "    Key insight: Use online softmax to avoid storing attention weights!\n",
    "    \"\"\"\n",
    "    # YOUR IMPLEMENTATION GOES HERE\n",
    "    # This is more complex - see Algorithm 1 in https://arxiv.org/pdf/2307.08691\n",
    "    pass\n",
    "\n",
    "\n",
    "def attention_triton_v2(Q, K, V, causal=False):\n",
    "    \"\"\"Flash Attention style implementation.\"\"\"\n",
    "    assert Q.shape == K.shape\n",
    "    assert K.shape[-1] == V.shape[-1]\n",
    "    B, H, N, d = Q.shape\n",
    "    scale = 1.0 / (d**0.5)\n",
    "\n",
    "    # Allocate output\n",
    "    Out = torch.empty_like(Q).contiguous()\n",
    "\n",
    "    # Ensure contiguity\n",
    "    Q = Q.contiguous()\n",
    "    K = K.contiguous()\n",
    "    V = V.contiguous()\n",
    "    \n",
    "    # Block sizes\n",
    "    BLOCK_M = min(triton.next_power_of_2(N), 64)\n",
    "    BLOCK_N = min(triton.next_power_of_2(N), 64)\n",
    "    BLOCK_D = d  # = d for simplicity\n",
    "    \n",
    "    grid = (triton.cdiv(N, BLOCK_M), B * H)\n",
    "    \n",
    "    attention_kernel_v2[grid](\n",
    "        Q, K, V, Out,\n",
    "        B, H, N, d,\n",
    "        scale,\n",
    "        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),\n",
    "        K.stride(0), K.stride(1), K.stride(2), K.stride(3),\n",
    "        V.stride(0), V.stride(1), V.stride(2), V.stride(3),\n",
    "        Out.stride(0), Out.stride(1), Out.stride(2), Out.stride(3),\n",
    "        causal,\n",
    "        BLOCK_M, \n",
    "        BLOCK_N, \n",
    "        BLOCK_D,\n",
    "    )\n",
    "\n",
    "    return Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcfc69-3c75-4b7d-9d2e-9b81f69b1f91",
   "metadata": {},
   "source": [
    "## Solution 🧙\n",
    "\n",
    "You shall not pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4130b2d-c465-4c02-8b4a-59a03bb5603b",
   "metadata": {},
   "source": [
    "### Version 1: Simple Block-wise Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192fcf43-0c45-45db-9cb2-7e5183a6e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our implementation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f41b4-2ea1-4139-9741-b260374b47ff",
   "metadata": {},
   "source": [
    "### Version 2: Flash Attention Style (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0bc8b2-ab68-433f-a1e9-3d2599ebd514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our implementation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6530fb-99db-4e08-b0e6-597162a03bf6",
   "metadata": {},
   "source": [
    "## Testing Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d1c7c-2c57-4946-9c2a-3082715f2421",
   "metadata": {},
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755af87-4b5a-484e-9807-6e7996a2504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attention(B=2, H=4, N=128, d=64, causal=False, atol=1e-2, rtol=1e-2):\n",
    "    \"\"\"Test attention implementation against PyTorch.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Generate random inputs\n",
    "    Q = torch.randn(B, H, N, d, device=DEVICE, dtype=torch.float32)\n",
    "    K = torch.randn(B, H, N, d, device=DEVICE, dtype=torch.float32)\n",
    "    V = torch.randn(B, H, N, d, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    # PyTorch reference\n",
    "    expected = attention_naive(Q, K, V, causal=causal)\n",
    "    \n",
    "    # Triton implementation\n",
    "    actual = attention_triton_v1(Q, K, V, causal=causal)\n",
    "    \n",
    "    try:\n",
    "        torch.testing.assert_close(actual, expected, atol=atol, rtol=rtol)\n",
    "        print(f\"✅ Test PASSED! ({'causal' if causal else 'non-causal'})\")\n",
    "        print(f\"   Shape: (B={B}, H={H}, N={N}, d={d})\")\n",
    "        print(f\"   Max error: {(actual - expected).abs().max().item():.2e}\")\n",
    "        return True\n",
    "    except AssertionError as e:\n",
    "        print(f\"❌ Test FAILED!\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test both causal and non-causal\n",
    "test_passed = test_attention(causal=False)\n",
    "test_passed &= test_attention(causal=True)\n",
    "\n",
    "# Display congrats message\n",
    "if test_passed:\n",
    "    print(\"\\n🎉 Congratulations! Your implementation is correct!\")\n",
    "    display(Image(\"figs/success.gif\", width=256, height=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd145430-ff9d-473e-ba4a-1aea1bcee822",
   "metadata": {},
   "source": [
    "### Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001561cc-721d-4209-ac62-82aa7dcd1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attention(B=2, H=4, N=128, d=64, causal=False, atol=1e-2, rtol=1e-2):\n",
    "    \"\"\"Test attention implementation against PyTorch.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Generate random inputs\n",
    "    Q = torch.randn(B, H, N, d, device=DEVICE, dtype=torch.float32)\n",
    "    K = torch.randn(B, H, N, d, device=DEVICE, dtype=torch.float32)\n",
    "    V = torch.randn(B, H, N, d, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    # PyTorch reference\n",
    "    expected = attention_naive(Q, K, V, causal=causal)\n",
    "    \n",
    "    # Triton implementation\n",
    "    actual = attention_triton_v2(Q, K, V, causal=causal)\n",
    "    \n",
    "    try:\n",
    "        torch.testing.assert_close(actual, expected, atol=atol, rtol=rtol)\n",
    "        print(f\"✅ Test PASSED! ({'causal' if causal else 'non-causal'})\")\n",
    "        print(f\"   Shape: (B={B}, H={H}, N={N}, d={d})\")\n",
    "        print(f\"   Max error: {(actual - expected).abs().max().item():.2e}\")\n",
    "        return True\n",
    "    except AssertionError as e:\n",
    "        print(f\"❌ Test FAILED!\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test both causal and non-causal\n",
    "test_passed = test_attention(causal=False)\n",
    "test_passed &= test_attention(causal=True)\n",
    "\n",
    "# Display congrats message\n",
    "if test_passed:\n",
    "    print(\"\\n🎉 Congratulations! Your implementation is correct!\")\n",
    "    display(Image(\"figs/success.gif\", width=256, height=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a807d87-85ab-4100-8860-aa940b2e4c69",
   "metadata": {},
   "source": [
    "## FLOP and Memory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c293b5b-d759-468a-a17b-9c05fe1fbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention(B, H, N, d):\n",
    "    \"\"\"Analyze memory and compute for attention.\"\"\"\n",
    "    element_size = 4  # float32\n",
    "    \n",
    "    # Standard attention\n",
    "    standard = {\n",
    "        'attention_matrix': B * H * N * N * element_size,  # Q @ K^T\n",
    "        'total_memory': B * H * N * N * element_size + B * H * N * d * element_size,\n",
    "    }\n",
    "    \n",
    "    # Flash attention (approximate)\n",
    "    block_size = 128\n",
    "    flash = {\n",
    "        'working_memory': B * H * block_size * block_size * element_size,  # One block at a time\n",
    "        'total_memory': B * H * block_size * block_size * element_size,\n",
    "    }\n",
    "    \n",
    "    # Compute\n",
    "    flops = B * H * (2 * N * N * d + 2 * N * N * d)  # Q@K^T + softmax@V\n",
    "    \n",
    "    return {\n",
    "        'standard_memory': standard['total_memory'],\n",
    "        'flash_memory': flash['total_memory'],\n",
    "        'memory_reduction': standard['total_memory'] / flash['total_memory'],\n",
    "        'flops': flops,\n",
    "    }\n",
    "\n",
    "# Example\n",
    "B, H, N, d = 8, 16, 512, 64\n",
    "analysis = analyze_attention(B, H, N, d)\n",
    "\n",
    "print(f\"Attention Analysis (B={B}, H={H}, N={N}, d={d}):\")\n",
    "print(f\"  Standard memory: {analysis['standard_memory'] / 1e9:.2f} GB\")\n",
    "print(f\"  Flash memory: {analysis['flash_memory'] / 1e6:.2f} MB\")\n",
    "print(f\"  Memory reduction: {analysis['memory_reduction']:.0f}x\")\n",
    "print(f\"  FLOPs: {analysis['flops'] / 1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53934712-14d3-4b68-87a5-9d55a95ca867",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully implemented the (forward pass) of Flash Attention - one of the cornerstone of modern AI!\n",
    "\n",
    "\n",
    "### Performance Insights:\n",
    "\n",
    "- **Memory bottleneck**: O(N²) attention matrix is the main constraint\n",
    "- **Flash Attention**: Never materializing full attention saves massive memory\n",
    "- **Recomputation**: Trading compute for memory is often worthwhile\n",
    "- **Block sizes**: Critical for balancing parallelism and memory usage\n",
    "\n",
    "### Real-World Impact:\n",
    "\n",
    "- **Longer sequences**: Flash Attention enables processing much longer sequences\n",
    "- **Larger models**: Memory savings allow bigger batch sizes\n",
    "- **Energy efficiency**: Less memory movement = less energy consumption\n",
    "- **Production ready**: These techniques power ChatGPT, Claude, and other LLMs!\n",
    "\n",
    "### Advanced Extensions:\n",
    "\n",
    "- **Multi-Query Attention (MQA)**: Share keys/values across heads\n",
    "- **Grouped-Query Attention (GQA)**: Share keys/values across groups of heads\n",
    "- **Sliding Window Attention**: Local attention patterns\n",
    "- **Sparse Attention**: Attend to specific positions only\n",
    "\n",
    "\n",
    "<center>\n",
    "    <b><span style=\"font-size: 24px\">Flash Swim, Sardine!</span></b>\n",
    "    <br>\n",
    "    <img src=\"figs/sardine-super-swim.png\" width=\"512\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe512c9-3727-4d8b-b1a0-451c31f9d5bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmarking (GPU only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5d8d2-9b2c-49fc-95f6-650b495b145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # Sequence length\n",
    "        x_vals=[128, 256, 512, 1024, 2048, 4096],  #, 8192, 16384, 32768, 65536],\n",
    "        line_arg='provider',\n",
    "        line_vals=['naive', 'pytorch', 'compiled', 'triton_v1', 'triton_v2'],\n",
    "        line_names=['Naive', 'PyTorch', 'Compiled', 'Triton v1', 'Triton v2'],\n",
    "        styles=[('blue', '-'), ('green', '--'), ('orange', '-.'), ('red', ':'), ('black', ':')],\n",
    "        ylabel='TFLOPS',\n",
    "        plot_name='attention-performance',\n",
    "        args={'B': 4, 'H': 8, 'd': 64, 'causal': True},  # feel free to edit these values!\n",
    "    )\n",
    ")\n",
    "def benchmark(N, B, H, d, causal, provider):\n",
    "    \"\"\"Benchmark attention implementations.\"\"\"\n",
    "    Q = torch.randn(B, H, N, d, device=DEVICE, dtype=torch.float32)\n",
    "    K = torch.randn(B, H, N, d, device=DEVICE, dtype=torch.float32)\n",
    "    V = torch.randn(B, H, N, d, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    # Normalize\n",
    "    Q = Q / Q.norm(dim=-1, keepdim=True)\n",
    "    K = K / K.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    quantiles = [0.5, 0.05, 0.95]\n",
    "    \n",
    "    if provider == 'naive':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: attention_naive(Q, K, V, causal=causal), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'pytorch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: attention_pytorch(Q, K, V, causal=causal), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'compiled':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: attention_compiled(Q, K, V, causal=causal), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'triton_v1':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: attention_triton_v1(Q, K, V, causal=causal), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'triton_v2':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: attention_triton_v2(Q, K, V, causal=causal), quantiles=quantiles\n",
    "        )\n",
    "    \n",
    "    # Calculate FLOPS (approximate)\n",
    "    flops = B * H * (2 * N * N * d + 2 * N * N * d)\n",
    "    tflops = lambda ms: flops / ms / 1e9\n",
    "    \n",
    "    return tflops(ms), tflops(max_ms), tflops(min_ms)\n",
    "\n",
    "print(\"Running benchmarks...\")\n",
    "results = benchmark.run(show_plots=True, print_data=True, return_df=True, save_path='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f0e756-eb25-47c0-8e3a-e110732c7189",
   "metadata": {},
   "source": [
    "## Speedup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6804c5-ed59-4c85-b9c2-c114b1a66131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Triton is faster than PyTorch\n",
    "avg_pytorch = results['PyTorch'].mean()\n",
    "avg_triton = results['Triton'].mean()\n",
    "speedup = avg_triton / avg_pytorch\n",
    "\n",
    "if speedup > 1.0:\n",
    "    print(f\"\\n🚀 Awesome! Triton is {speedup:.2f}x faster than PyTorch!\")\n",
    "    display(Image(\"figs/gpu.gif\", width=400, height=256))\n",
    "else:\n",
    "    print(f\"\\n🐌 Not quite yet! Triton implementation is {speedup:.2f}x slower than PyTorch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23991162-8a60-4018-87ed-7bb19bf587e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
