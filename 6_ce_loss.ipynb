{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab2b3b-b5be-4f71-973d-bbcc6787bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_triton import setup_triton\n",
    "\n",
    "# TRITON_INTERPRET=1 uses a python interpreter instead of running on the GPU. \n",
    "# This menas that uou can insert Python breakpoints to debug your kernel code! \n",
    "setup_triton(use_interpreter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c207c79-abf5-46f5-bc28-e5d79b3cb9aa",
   "metadata": {},
   "source": [
    "# Triton Puzzle 6: Fused Cross Entropy Loss\n",
    "\n",
    "Welcome to the sixth Triton puzzle! Cross entropy loss is fundamental to classification tasks. This puzzle is special because it introduces **PyTorch autograd integration**. That means you'll implement both forward and backward passes!\n",
    "\n",
    "### What you'll learn:\n",
    "\n",
    "- **Fusing multiple operations** for efficiency (log-softmax + nll_loss)\n",
    "- **PyTorch autograd integration** using `torch.autograd.Function`\n",
    "- **Backward pass implementation** for gradient computation\n",
    "- **Numerical stability** at extreme values\n",
    "- **Memory efficiency** by avoiding intermediate tensors\n",
    "- **Interoperability** between Triton kernels and PyTorch's autograd\n",
    "\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "**Cross-entropy loss** for a single example with logits $\\mathbf{x}\\in\\mathbb{R}^C$ and ground-truth class label $y\\in\\{0,\\dots,C-1\\}$ is\n",
    "\n",
    "$$\n",
    "\\mathcal L(\\mathbf{x},y)\n",
    "\\;=\\;\n",
    "-\\log\\frac{e^{x_{y}}}{\\sum_{j=0}^{C-1} e^{x_{j}}}\n",
    "\\;=\\;\n",
    "-\\;x_{y} + \\log\\left(\\sum_{j=0}^{C-1} e^{x_{j}}\\right).\n",
    "$$\n",
    "\n",
    "> **Implementation Note**: The form $-x_y + \\log(\\sum_j e^{x_j})$ is key for numerical stability. In the Triton kernel, we compute the log-sum-exp term using the max trick: $\\max(\\mathbf{x}) + \\log(\\sum_j e^{x_j - \\max(\\mathbf{x})})$.\n",
    "\n",
    "For a mini-batch of $N$ examples, we stack examples as rows: $X\\in\\mathbb{R}^{N\\times C}$ with labels $\\mathbf{y}\\in\\{0,\\dots,C-1\\}^N$. With **mean reduction**:\n",
    "\n",
    "$$\n",
    "\\boxed{\\;\n",
    "\\mathcal L(X,\\mathbf{y})=\\frac{1}{N}\\sum_{n=0}^{N-1}\n",
    "\\mathcal L\\left(\\mathbf{x}^{(n)},y_n\\right)\n",
    "\\;}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}^{(n)}$ denotes the $n$-th row of $X$ (logits for the $n$-th example).\n",
    "\n",
    "> **Memory Efficiency**: Our Triton kernel computes this loss without ever materializing the $N \\times C$ softmax probability matrix!\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Gradient Derivation (single example)\n",
    "\n",
    "The gradient computation is crucial for backpropagation and reveals why this operation fuses so beautifully:\n",
    "\n",
    "1. **Softmax probabilities**\n",
    "   $$\n",
    "   p_i\n",
    "   =\\operatorname{softmax}(\\mathbf{x})_i\n",
    "   =\\frac{e^{x_i}}{\\sum_{j=0}^{C-1}e^{x_j}}.\n",
    "   $$\n",
    "\n",
    "2. **Derivative of the log-sum-exp term**\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial x_i}\n",
    "   \\log\\left(\\sum_{j}e^{x_j}\\right)\n",
    "   =\\frac{e^{x_i}}{\\sum_{j}e^{x_j}}\n",
    "   =p_i.\n",
    "   $$\n",
    "   \n",
    "   > **Key insight**: The gradient requires the softmax probabilities $p_i$, which we can recompute in the backward pass rather than storing them!\n",
    "\n",
    "3. **Combine pieces**\n",
    "   $$\n",
    "   \\frac{\\partial\\mathcal L}{\\partial x_i}\n",
    "   =\\frac{\\partial}{\\partial x_i}\n",
    "     \\left[-x_y+\\log\\left(\\sum_{j}e^{x_j}\\right)\\right]\n",
    "   =-\\,\\delta_{iy}+p_i\n",
    "   =\\begin{cases}\n",
    "        p_i-1,& i=y,\\\\[6pt]\n",
    "        p_i,& i\\neq y.\n",
    "     \\end{cases}\n",
    "   $$\n",
    "   \n",
    "   where $\\delta_{iy} = 1$ if $i = y$, else $0$ (Kronecker delta).\n",
    "\n",
    "4. **Vector form**\n",
    "   Let $\\mathbf{y}^{\\text{onehot}}\\in\\{0,1\\}^C$ with $(\\mathbf{y}^{\\text{onehot}})_i=\\delta_{iy}$. Then:\n",
    "   \n",
    "   $$\n",
    "   \\boxed{\\;\n",
    "   \\nabla_{\\mathbf{x}}\\mathcal L\n",
    "   =\\mathbf{p}-\\mathbf{y}^{\\text{onehot}}\n",
    "   \\;}\n",
    "   $$\n",
    "   \n",
    "   <!-- > **Triton Implementation**: In the backward kernel, we compute this as: -->\n",
    "   <!-- > ```python -->\n",
    "   <!-- > softmax = tl.exp(logits - max_logit) / sum_exp -->\n",
    "   <!-- > grad = softmax - tl.where(offs == label, 1.0, 0.0) -->\n",
    "   <!-- > ``` -->\n",
    "\n",
    "---\n",
    "\n",
    "### Extension to a Mini-Batch with Chain Rule\n",
    "\n",
    "For a batch, define:\n",
    "- $P = \\operatorname{softmax}(X)$ (row-wise softmax)\n",
    "- $Y \\in \\{0,1\\}^{N\\times C}$ (one-hot encoding of labels)\n",
    "\n",
    "For **mean reduction** loss with upstream gradient $\\mathbf{g} \\in \\mathbb{R}^N$:\n",
    "\n",
    "$$\n",
    "\\boxed{\\;\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{total}}}{\\partial X_{ij}}\n",
    "= \\frac{1}{N} \\cdot g_i \\cdot (P_{ij} - Y_{ij})\n",
    "\\;}\n",
    "$$\n",
    "\n",
    "> **Chain Rule in Practice**: The $g_i$ term (grad_output in PyTorch) scales each example's gradient.\n",
    "<!-- > In our Triton kernel: -->\n",
    "<!-- > ```python -->\n",
    "<!-- > grad = (softmax - one_hot) * grad_output[row] -->\n",
    "<!-- > ``` -->\n",
    "\n",
    "---\n",
    "\n",
    "### Numerical Stability: The Log-Sum-Exp Trick\n",
    "\n",
    "Direct computation of $\\log(\\sum_j e^{x_j})$ can overflow. Instead, we use:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\sum_{j} e^{x_j}\\right) = \\max(\\mathbf{x}) + \\log \\left(\\sum_{j} e^{x_j - \\max(\\mathbf{x})}\\right)\n",
    "$$\n",
    "\n",
    "This is more numerically stable.\n",
    "\n",
    "<!-- > **Triton Code Alignment**: -->\n",
    "<!-- > ```python -->\n",
    "<!-- > # First pass: find max -->\n",
    "<!-- > max_logit = tl.max(chunk) -->\n",
    "<!-- > # Second pass: compute stable log-sum-exp   -->\n",
    "<!-- > exp_chunk = tl.exp(chunk - max_logit) -->\n",
    "<!-- > sum_exp += tl.sum(exp_chunk) -->\n",
    "<!-- > log_sum_exp = max_logit + tl.log(sum_exp) -->\n",
    "<!-- > ``` -->\n",
    "\n",
    "---\n",
    "\n",
    "### Why Fuse? Memory and Computation Analysis\n",
    "\n",
    "**Unfused approach** (naive):\n",
    "1. Compute softmax → store $N \\times C$ tensor\n",
    "2. Take log → store $N \\times C$ tensor  \n",
    "3. Select correct class → store $N$ tensor\n",
    "4. Negate → final loss\n",
    "\n",
    "**Memory traffic**: $3NC + N$ elements\n",
    "\n",
    "**Fused approach** (our implementation):\n",
    "- Single pass through input\n",
    "- Only store final loss ($N$ elements)\n",
    "- Recompute softmax in backward pass\n",
    "\n",
    "**Memory traffic**: $NC + N$ elements (forward), $NC + NC$ elements (backward)\n",
    "\n",
    "> **Performance Impact**: For typical values (N=1024, C=50000), fusion reduces memory traffic by ~3x, which directly translates to faster execution on memory-bound GPUs!\n",
    "\n",
    "**💡 Final Tip**: Remember that you can use `TRITON_INTERPRET=1` to debug your kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaad09d-4675-421e-80a4-637ff4c09b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d47dd-6375-4140-8f79-091673bbc5d1",
   "metadata": {},
   "source": [
    "## Implementation 1: Naive PyTorch\n",
    "\n",
    "First, let's see the unfused approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a634d-094d-4a1a-9d1f-5c8a83801741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_naive(logits, labels):\n",
    "    \"\"\"\n",
    "    Naive unfused implementation.\n",
    "    This creates multiple intermediate tensors.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute softmax\n",
    "    softmax = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Step 2: Take log\n",
    "    log_softmax = torch.log(softmax)\n",
    "    \n",
    "    # Step 3: Gather correct class and negate\n",
    "    batch_size = logits.shape[0]\n",
    "    losses = -log_softmax[torch.arange(batch_size), labels]\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34aaa4-3db1-48e3-9506-ed1e8ffb1ff2",
   "metadata": {},
   "source": [
    "## Implementation 2: PyTorch Built-in\n",
    "\n",
    "PyTorch's built-in is already optimized and fused:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aef1e0-d629-4dd7-90e7-415ad7f97c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_pytorch(logits, labels, reduction='none'):\n",
    "    \"\"\"PyTorch's built-in cross entropy.\"\"\"\n",
    "    return torch.nn.functional.cross_entropy(logits, labels, reduction=reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ae985-8341-4422-b133-a945cf12bd26",
   "metadata": {},
   "source": [
    "## Implementation 3: PyTorch Compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2de80-0783-4482-aa94-cca2426a20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def cross_entropy_compiled(logits, labels):\n",
    "    \"\"\"\n",
    "    Naive unfused implementation.\n",
    "    This creates multiple intermediate tensors.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute softmax\n",
    "    softmax = torch.softmax(logits, dim=-1)\n",
    "    # Step 2: Take log\n",
    "    log_softmax = torch.log(softmax)\n",
    "    # Step 3: Gather correct class and negate\n",
    "    batch_size = logits.shape[0]\n",
    "    losses = -log_softmax[torch.arange(batch_size), labels]\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6cc052-3351-4546-811b-dc602d3a599e",
   "metadata": {},
   "source": [
    "## Key Concepts for This Puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ddc419-3d82-49a2-afd4-0c0a6a606622",
   "metadata": {},
   "source": [
    "### 1. PyTorch Autograd Integration\n",
    "\n",
    "To integrate with PyTorch's autograd:\n",
    "\n",
    "```python\n",
    "class CustomFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, *args, **kwargs):\n",
    "        # Save tensors needed for backward\n",
    "        ctx.save_for_backward(input, *args, **kwargs)\n",
    "        # Compute forward pass\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve saved tensors\n",
    "        input, extras = ctx.saved_tensors\n",
    "        # Compute gradients\n",
    "        return grad_input, *extras\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb039c-3a8c-43c4-aee8-7c2e1c929283",
   "metadata": {},
   "source": [
    "### 2. Numerical Stability\n",
    "\n",
    "Use the log-sum-exp trick:\n",
    "- Instead of: `log(sum(exp(x)))`\n",
    "- Compute: `max(x) + log(sum(exp(x - max(x))))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65537fc5-c187-4523-8693-1a68eb4bb857",
   "metadata": {},
   "source": [
    "### 3. Memory Considerations\n",
    "\n",
    "- Forward: Only save what's needed for backward (logits and labels)\n",
    "- Backward: Compute softmax again rather than storing it\n",
    "- This trades computation for memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832f38a-952d-45fd-ac92-8d25f3754142",
   "metadata": {},
   "source": [
    "## Implementation 4: Triton Kernels (Puzzle)\n",
    "\n",
    "Now implement both forward and backward kernels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbd8be-5361-4f6e-b340-8ca126041934",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def cross_entropy_forward_kernel(\n",
    "    logits_ptr, labels_ptr, losses_ptr,\n",
    "    N, C,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    \"\"\"\n",
    "    Forward kernel for cross entropy loss.\n",
    "    Each program handles one sample.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load all logits for this sample\n",
    "    2. Compute max for numerical stability\n",
    "    3. Compute log-sum-exp\n",
    "    4. Compute loss = log-sum-exp - logit[label]\n",
    "    \"\"\"\n",
    "    # YOUR IMPLEMENTATION GOES HERE\n",
    "    # Hints:\n",
    "    # 1. Use tl.program_id(0) to get sample index\n",
    "    # 2. Load all C logits for this sample (loop if C > BLOCK_SIZE)\n",
    "    # 3. Find max using tl.max\n",
    "    # 4. Compute stable log-sum-exp\n",
    "    # 5. Load the label and compute final loss\n",
    "    pass\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def cross_entropy_backward_kernel(\n",
    "    grad_output_ptr, logits_ptr, labels_ptr, grad_input_ptr,\n",
    "    N, C,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    \"\"\"\n",
    "    Backward kernel for cross entropy loss.\n",
    "    Each program handles one sample.\n",
    "    \n",
    "    Gradient formula: softmax(logits) - one_hot(label)\n",
    "    Scaled by grad_output (chain rule)\n",
    "    \"\"\"\n",
    "    # YOUR IMPLEMENTATION GOES HERE\n",
    "    # Hints:\n",
    "    # 1. Recompute softmax (don't store it from forward!)\n",
    "    # 2. Subtract 1 from the softmax value at the label position\n",
    "    # 3. Multiply by grad_output for chain rule\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07010731-225c-4301-bbfe-01fcd107eb22",
   "metadata": {},
   "source": [
    "#### PyTorch Function Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874523e-9aeb-4407-8c93-f9b7e326ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedCrossEntropyFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits, labels):\n",
    "        \"\"\"\n",
    "        Forward pass of fused cross entropy.\n",
    "        \n",
    "        Args:\n",
    "            logits: (N, C) tensor of logits\n",
    "            labels: (N,) tensor of class indices\n",
    "            \n",
    "        Returns:\n",
    "            losses: (N,) tensor of losses\n",
    "        \"\"\"\n",
    "        assert logits.is_contiguous()\n",
    "        assert labels.is_contiguous()\n",
    "        assert labels.dtype == torch.long\n",
    "        \n",
    "        N, C = logits.shape\n",
    "        \n",
    "        # Allocate output\n",
    "        losses = torch.empty(N, device=logits.device, dtype=logits.dtype)\n",
    "        \n",
    "        # Choose block size\n",
    "        BLOCK_SIZE = triton.next_power_of_2(min(C, 1024))\n",
    "        \n",
    "        # Launch forward kernel\n",
    "        grid = (N,)\n",
    "        cross_entropy_forward_kernel[grid](\n",
    "            logits, labels, losses,\n",
    "            N, C,\n",
    "            BLOCK_SIZE\n",
    "        )\n",
    "        \n",
    "        # Save for backward\n",
    "        ctx.save_for_backward(logits, labels)\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass of fused cross entropy.\n",
    "        \n",
    "        Args:\n",
    "            grad_output: (N,) gradient w.r.t. losses\n",
    "            \n",
    "        Returns:\n",
    "            grad_logits: (N, C) gradient w.r.t. logits\n",
    "            grad_labels: None (labels don't need gradients)\n",
    "        \"\"\"\n",
    "        logits, labels = ctx.saved_tensors\n",
    "        N, C = logits.shape\n",
    "        \n",
    "        # Allocate gradient and ensure they are contiguous\n",
    "        grad_logits = torch.zeros_like(logits).contiguous()\n",
    "        grad_output = grad_output.contiguous()\n",
    "        \n",
    "        # Choose block size\n",
    "        BLOCK_SIZE = triton.next_power_of_2(min(C, 1024))\n",
    "\n",
    "        # Launch backward kernel\n",
    "        grid = (N,)\n",
    "        cross_entropy_backward_kernel[grid](\n",
    "            grad_output, logits, labels, grad_logits,\n",
    "            N, C,\n",
    "            BLOCK_SIZE\n",
    "        )\n",
    "        \n",
    "        return grad_logits, None\n",
    "\n",
    "# Convenience function\n",
    "def cross_entropy_triton(logits, labels):\n",
    "    \"\"\"Fused cross entropy loss using Triton.\"\"\"\n",
    "    return FusedCrossEntropyFunction.apply(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5475b44-8501-4db2-9065-85771ab879f4",
   "metadata": {},
   "source": [
    "## Solution 🧙\n",
    "\n",
    "You shall not pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc88eb0-4e2c-4b82-b681-a3d2e9658094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our implementation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd65ac-0d2d-4dce-b43e-90e47a54f385",
   "metadata": {},
   "source": [
    "## Testing Correctness\n",
    "\n",
    "Let's test both forward and backward passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba4904-6a6f-4366-81f1-bb539f0889b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_pass(N=32, C=100, atol=1e-5, rtol=1e-5):\n",
    "    \"\"\"Test forward pass against PyTorch.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Create test data\n",
    "    logits = torch.randn(N, C, device=DEVICE, requires_grad=True)\n",
    "    labels = torch.randint(0, C, (N,), device=DEVICE)\n",
    "    \n",
    "    # PyTorch reference\n",
    "    expected = cross_entropy_pytorch(logits, labels, reduction='none')\n",
    "    \n",
    "    # Triton implementation\n",
    "    actual = cross_entropy_triton(logits, labels)\n",
    "    \n",
    "    # Check correctness\n",
    "    try:\n",
    "        torch.testing.assert_close(actual, expected, atol=atol, rtol=rtol)\n",
    "        print(f\"✅ Forward pass PASSED!\")\n",
    "        print(f\"   Shape: ({N}, {C})\")\n",
    "        print(f\"   Max error: {(actual - expected).abs().max().item():.2e}\")\n",
    "        return True\n",
    "    except AssertionError as e:\n",
    "        print(f\"❌ Forward pass FAILED!\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_backward_pass(N=32, C=100, atol=1e-4, rtol=1e-4):\n",
    "    \"\"\"Test backward pass against PyTorch.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Create test data\n",
    "    logits = torch.randn(N, C, device=DEVICE, dtype=torch.float32, requires_grad=True)\n",
    "    labels = torch.randint(0, C, (N,), device=DEVICE)\n",
    "    print(labels)\n",
    "    \n",
    "    # Use gradcheck to verify gradients\n",
    "    try:\n",
    "        # Also compare against PyTorch\n",
    "        logits_torch = logits.detach().clone().requires_grad_(True)\n",
    "        logits_triton = logits.detach().clone().requires_grad_(True)\n",
    "        \n",
    "        # Forward\n",
    "        loss_torch = cross_entropy_pytorch(logits_torch, labels, reduction='none').sum()\n",
    "        loss_triton = cross_entropy_triton(logits_triton, labels).sum()\n",
    "        \n",
    "        # Backward\n",
    "        loss_torch.backward()\n",
    "        loss_triton.backward()\n",
    "        \n",
    "        # Compare gradients\n",
    "        torch.testing.assert_close(logits_triton.grad, logits_torch.grad, atol=atol, rtol=rtol)\n",
    "        print(f\"✅ Gradient comparison PASSED!\")\n",
    "        print(f\"   Max gradient error: {(logits_triton.grad - logits_torch.grad).abs().max().item():.2e}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Backward pass FAILED!\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run tests\n",
    "forward_passed = test_forward_pass()\n",
    "backward_passed = test_backward_pass()\n",
    "test_passed = forward_passed and backward_passed\n",
    "\n",
    "# Display congrats message\n",
    "if test_passed:\n",
    "    print(\"\\n🎉 Congratulations! Your implementation is correct!\")\n",
    "    display(Image(\"figs/success.gif\", width=256, height=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0dc2c3-3ba9-4bdc-aa71-23ddd278a3be",
   "metadata": {},
   "source": [
    "## FLOP and Memory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046973d9-ac29-4335-b7be-3f28b9127ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_entropy(N, C):\n",
    "    \"\"\"Analyze memory and compute for cross entropy.\"\"\"\n",
    "    element_size = 4  # float32\n",
    "    \n",
    "    # Unfused approach\n",
    "    unfused = {\n",
    "        'softmax_store': N * C * element_size,      # Store softmax\n",
    "        'log_softmax_store': N * C * element_size,  # Store log_softmax\n",
    "        'memory_traffic': 3 * N * C * element_size,  # Read logits + 2 intermediate writes\n",
    "    }\n",
    "    \n",
    "    # Fused approach\n",
    "    fused = {\n",
    "        'memory_traffic': N * C * element_size + N * element_size,  # Read logits + write losses\n",
    "    }\n",
    "    \n",
    "    # Compute\n",
    "    flops_per_sample = C + C + 1  # exp + sum + log (simplified)\n",
    "    total_flops = N * flops_per_sample\n",
    "    \n",
    "    return {\n",
    "        'unfused_memory': unfused['memory_traffic'],\n",
    "        'fused_memory': fused['memory_traffic'],\n",
    "        'memory_reduction': unfused['memory_traffic'] / fused['memory_traffic'],\n",
    "        'flops': total_flops,\n",
    "        'ai_fused': total_flops / fused['memory_traffic']\n",
    "    }\n",
    "\n",
    "# Example\n",
    "N, C = 1024, 1000\n",
    "analysis = analyze_cross_entropy(N, C)\n",
    "print(f\"Cross Entropy {N}x{C}:\")\n",
    "print(f\"  Unfused memory: {analysis['unfused_memory'] / 1e6:.2f} MB\")\n",
    "print(f\"  Fused memory: {analysis['fused_memory'] / 1e6:.2f} MB\")\n",
    "print(f\"  Memory reduction: {analysis['memory_reduction']:.1f}x\")\n",
    "print(f\"  Arithmetic intensity: {analysis['ai_fused']:.2f} FLOPs/byte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe88f9e-e534-4767-b09d-77e9035edafd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully implemented a fused cross entropy loss with full autograd support!\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "\n",
    "1. **Operation Fusion**: Combining log-softmax + NLL for efficiency\n",
    "2. **PyTorch Integration**: Using `torch.autograd.Function`\n",
    "3. **Gradient Computation**: Efficient backward pass implementation\n",
    "4. **Numerical Stability**: Log-sum-exp trick for large values\n",
    "5. **Memory Efficiency**: Avoiding intermediate tensor storage\n",
    "\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "You're ready for the final challenge: Flash Attention!\n",
    "\n",
    "<img src=\"figs/sardine-flash.png\" width=\"512\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167beb28-754b-46ba-9d04-fdbfc1bb8e8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmarking (GPU only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf720002-164c-42a6-90d3-15f002e85d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['C'],  # Number of classes\n",
    "        x_vals=[2**i for i in range(8, 17)],\n",
    "        line_arg='provider',\n",
    "        line_vals=['pytorch', 'compile', 'triton'],\n",
    "        line_names=['PyTorch', 'torch.compile', 'Triton'],\n",
    "        styles=[('green', '-'), ('red', '--'), ('blue', '-.')],\n",
    "        ylabel='GB/s',\n",
    "        plot_name='cross-entropy-performance',\n",
    "        args={'N': 32},\n",
    "    )\n",
    ")\n",
    "def benchmark(C, N, provider):\n",
    "    \"\"\"Benchmark cross entropy implementations.\"\"\"\n",
    "    logits = torch.randn(N, C, device=DEVICE, dtype=torch.float32)\n",
    "    labels = torch.randint(0, C, (N,), device=DEVICE)\n",
    "    \n",
    "    quantiles = [0.5, 0.05, 0.95]\n",
    "    \n",
    "    if provider == 'pytorch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: cross_entropy_pytorch(logits, labels), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: cross_entropy_compiled(logits, labels), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: cross_entropy_triton(logits, labels), quantiles=quantiles\n",
    "        )\n",
    "    \n",
    "    # Calculate bandwidth (input + output)\n",
    "    bytes_moved = logits.numel() * logits.element_size()  # Read logits\n",
    "    bytes_moved += N * 4  # Write losses\n",
    "    \n",
    "    gb_per_s = lambda ms: bytes_moved / ms / 1e6\n",
    "    \n",
    "    return gb_per_s(ms), gb_per_s(max_ms), gb_per_s(min_ms)\n",
    "\n",
    "print(\"Running benchmarks...\")\n",
    "results = benchmark.run(show_plots=True, print_data=True, return_df=True, save_path='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07d24e0-a657-4af9-98dc-14a2114fd953",
   "metadata": {},
   "source": [
    "## Speedup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd07009-cb6f-4fe9-a71a-8fb5b6468759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Triton is faster than PyTorch\n",
    "avg_pytorch = results['PyTorch'].mean()\n",
    "avg_triton = results['Triton'].mean()\n",
    "speedup = avg_triton / avg_pytorch\n",
    "\n",
    "if speedup > 1.0:\n",
    "    print(f\"\\n🚀 Awesome! Triton is {speedup:.2f}x faster than PyTorch!\")\n",
    "    display(Image(\"figs/gpu.gif\", width=400, height=256))\n",
    "else:\n",
    "    print(f\"\\n🐌 Not quite yet! Triton implementation is {speedup:.2f}x slower than PyTorch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d7055-f833-4e20-8780-d9d69dabd175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
