{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "900327eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import triton\n",
                "import triton.language as tl\n",
                "\n",
                "# !pip install entmax\n",
                "from entmax import entmax_bisect "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "925a1804",
            "metadata": {},
            "source": [
                "### Goal: Implement entmax bisection in Triton\n",
                "\n",
                "The $\\alpha$-entmax mapping is given by:\n",
                "$$\n",
                "{\\alpha\\text{-entmax}(x)}_i = [(\\alpha - 1)x_i - \\tau]_+^{\\frac{1}{\\alpha-1}} \n",
                "$$\n",
                "\n",
                "Thus, for one to calculate entmax, one requires to find $\\tau$ that satistfies $\\sum_i [(\\alpha - 1)x_i - \\tau]_+^{\\frac{1}{\\alpha-1}} = 1$, that is, that satisfies that the sum of all entries sums to one.\n",
                "\n",
                "Let us define the function $f(\\tau)$ for which we are trying to find the root for:\n",
                "$$\n",
                "f(\\tau) = \\sum_i [(\\alpha - 1)x_i - \\tau]_+^{\\frac{1}{\\alpha-1}} - 1\n",
                "$$\n",
                "\n",
                "Currently, in the entmax package (pip install entmax) the algorithm to calculate this $\\tau$ makes use of the bisection algorithm. Thus, a high-level description of the algorithm goes like this:\n",
                "\n",
                "$$\n",
                "\\begin{align*}\n",
                "\\text{Input: }& x \\in \\mathbb{R}^n, \\alpha \\in \\mathbb{R}, T \\text{ iterations} \\\\\n",
                "\\text{(1): }& \\max \\leftarrow \\max(x) \\\\ \n",
                "\\text{(2): }& \\text{Initialize } \\tau_\\text{lo} = \\max - 1,\\tau_\\text{hi} = \\max - n^{1-\\alpha}, \\tau = \\frac{\\tau_\\text{lo} + \\tau_\\text{hi}}{2} \\\\\n",
                "\\text{(3): }& \\text{For t in T iterations do:} \\\\\n",
                "&\\quad\\text{Compute } f(\\tau) \\\\\n",
                "&\\quad\\text{If } f(\\tau) > 0: \\tau_\\text{lo} = \\tau \\text{ else } \\tau_\\text{hi} = \\tau \\\\\n",
                "&\\quad\\tau \\leftarrow \\frac{\\tau_\\text{lo} + \\tau_\\text{hi}}{2} \\\\\n",
                "\\text{(4): }& \\text{Store element-wise }  [(\\alpha - 1)x_i - \\tau]_+^{\\frac{1}{\\alpha-1}} \\\\\n",
                "\\end{align*}\n",
                "$$"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "46876d50",
            "metadata": {},
            "source": [
                "### Complete the code below. \n",
                "Assume the following:\n",
                "- The input is a matrix $b \\times n$, and we want to perform the entmax transformation along the last dimension. \n",
                "- $n$ is a power of two, so no masking is required."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cfd51564",
            "metadata": {},
            "outputs": [],
            "source": [
                "@triton.jit\n",
                "def _ent_bisect(x_ptr, y_ptr, alpha, n_iter, N: tl.constexpr, TILE: tl.constexpr):\n",
                "    \n",
                "    # YOUR IMPLEMENTATION GOES HERE\n",
                "    # 1) Calculate the maximum.\n",
                "    # 2) Run bisection `n_iter` times (be careful about possible NaNs!).\n",
                "    # 3) Finally apply the entmax function and store the result.\n",
                "    \n",
                "    pass\n",
                "\n",
                "def entmax_triton(x, alpha=1.5, n_iter=50):\n",
                "    rows, cols = x.shape\n",
                "    assert cols.bit_count() == 1, \"We require the number of columns to be a power of 2.\"\n",
                "    TILE = 1024 if cols > 1024 else cols\n",
                "\n",
                "    # launch with as many programs as rows in x\n",
                "    grid = (rows,)\n",
                "\n",
                "    # allocate output tensor\n",
                "    y = torch.empty_like(x)\n",
                "\n",
                "    # launch the kernel\n",
                "    _ent_bisect[grid](x, y, alpha, n_iter, cols, TILE)\n",
                "\n",
                "    return y"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6a2ab0ca",
            "metadata": {},
            "outputs": [],
            "source": [
                "b, n = 16, 4096 * 2\n",
                "alpha = 1.5\n",
                "n_iter = 50\n",
                "\n",
                "x = torch.randn((b, n), device='cuda', dtype=torch.float32).contiguous()\n",
                "y_ref = entmax_bisect(x, alpha=alpha, n_iter=50)\n",
                "y_triton = entmax_triton(x, alpha=alpha, n_iter=n_iter)\n",
                "\n",
                "print(f\"Max error: {torch.max(torch.abs(y_ref - y_triton))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c4438f97",
            "metadata": {},
            "source": [
                "### ⬇️ Below you can check how well your solution performs against entmax's package."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "83aa7a17",
            "metadata": {},
            "outputs": [],
            "source": [
                "@triton.testing.perf_report(\n",
                "    triton.testing.Benchmark(\n",
                "        x_names=['size'], \n",
                "        x_vals=[4096, 8192, 16384],\n",
                "        line_arg='provider', \n",
                "        line_vals=['triton', 'torch'],  \n",
                "        line_names=['Triton', 'Torch'],\n",
                "        styles=[('blue', '-'), ('green', '--')], \n",
                "        ylabel='Time (ms)', \n",
                "        plot_name='entmax-perf',\n",
                "        args={},\n",
                "    ))\n",
                "\n",
                "\n",
                "\n",
                "def benchmark(size, provider):\n",
                "    alpha = 1.5\n",
                "    n_iter = 20\n",
                "    x = torch.rand((2048, size), device=\"cuda\", dtype=torch.float32)\n",
                "\n",
                "    quantiles = [0.5, 0.2, 0.8]\n",
                "    if provider == 'torch':\n",
                "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: entmax_bisect(x, n_iter=n_iter, dim=1), quantiles=quantiles, warmup=500, rep=1000)\n",
                "    if provider == 'triton':\n",
                "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: entmax_triton(x, alpha=alpha, n_iter=n_iter), quantiles=quantiles, warmup=500, rep=1000)\n",
                "    return ms"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "20e6557b",
            "metadata": {},
            "outputs": [],
            "source": [
                "benchmark.run(print_data=True, show_plots=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e81be7e8",
            "metadata": {},
            "source": [
                "## ⚠️ Solution below!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "50895c74",
            "metadata": {},
            "outputs": [],
            "source": [
                "@triton.jit\n",
                "def alpha_entmax(x, tau, alpha):\n",
                "    x = (alpha - 1) * x - tau\n",
                "    # Here we have to mask out negative values\n",
                "    # because we are using log2, which is not defined for negative values.\n",
                "    x = tl.where(x > 0, tl.exp2(1 / (alpha - 1) * tl.log2(x)), 0.0)\n",
                "    return x\n",
                "\n",
                "\n",
                "@triton.jit\n",
                "def _ent_bisect(x_ptr, y_ptr, alpha, n_iter, N: tl.constexpr, TILE: tl.constexpr):\n",
                "    # get row that this thread block will be responsible for\n",
                "    curr_row = tl.program_id(0)\n",
                "\n",
                "    # move pointers to the start of the input and output tensors\n",
                "    x_ptr += curr_row * N\n",
                "    y_ptr += curr_row * N\n",
                "    \n",
                "    # same as torch.arange\n",
                "    offsets = tl.arange(0, TILE)\n",
                "\n",
                "    # placeholder for max value\n",
                "    max_val = -1.0e3\n",
                "\n",
                "    for idx in range(0, N, TILE):\n",
                "        # compute pointers for the current tile\n",
                "        x_ptrs = (x_ptr + idx) + offsets\n",
                "\n",
                "        # load TILE elements of X\n",
                "        x = tl.load(x_ptrs)\n",
                "\n",
                "        # update max value\n",
                "        max_val = tl.maximum(max_val, tl.max(x))\n",
                "\n",
                "    max_val *= (alpha - 1.0)\n",
                "\n",
                "    # initialize tau bounds\n",
                "    tau_lower = max_val - 1.0\n",
                "    tau_upper = max_val - tl.exp2((1-alpha) * tl.log2(1.0*N))\n",
                "    tau = (tau_lower + tau_upper) / 2.0\n",
                "    \n",
                "    # bisection\n",
                "    for _ in range(n_iter):\n",
                "        f_tau = -1.0\n",
                "\n",
                "        for idx in range(0, N, TILE):\n",
                "            # compute pointers for the current tile\n",
                "            x_ptrs = (x_ptr + idx) + offsets\n",
                "\n",
                "            # load TILE elements of X\n",
                "            x = tl.load(x_ptrs)\n",
                "\n",
                "            # accumulate f(tau)\n",
                "            f_tau += tl.sum(alpha_entmax(x, tau, alpha))\n",
                "\n",
                "        # update tau bounds\n",
                "        if f_tau > 0:\n",
                "            tau_lower = tau\n",
                "        else:\n",
                "            tau_upper = tau\n",
                "        tau = (tau_lower + tau_upper) / 2.0\n",
                "\n",
                "\n",
                "    for idx in range(0, N, TILE):\n",
                "            # compute pointers for the current tile\n",
                "            x_ptrs = (x_ptr + idx) + offsets\n",
                "            y_ptrs = (y_ptr + idx) + offsets\n",
                "\n",
                "            # load TILE elements of X\n",
                "            x = tl.load(x_ptrs)\n",
                "\n",
                "            # compute entmax for this TILE\n",
                "            y = alpha_entmax(x, tau, alpha)\n",
                "\n",
                "            # store results\n",
                "            tl.store(y_ptrs, y)\n",
                "\n",
                "def entmax_triton(x, alpha=1.5, n_iter=50):\n",
                "    rows, cols = x.shape\n",
                "    assert cols.bit_count() == 1, \"We require the number of columns to be a power of 2.\"\n",
                "    TILE = 1024 if cols > 1024 else cols\n",
                "\n",
                "    # launch with as many blocks as rows in x\n",
                "    grid = (rows,)\n",
                "\n",
                "    # allocate output tensor\n",
                "    y = torch.empty_like(x)\n",
                "\n",
                "    # launch the kernel\n",
                "    _ent_bisect[grid](x, y, alpha, n_iter, cols, TILE)\n",
                "\n",
                "    return y"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "20-08-latest",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
