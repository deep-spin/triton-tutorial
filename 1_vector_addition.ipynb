{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83d8bfd-3f3a-46dc-af7b-65e2d6d5a28c",
   "metadata": {},
   "source": [
    "# Triton Puzzle 1: Vector Addition\n",
    "\n",
    "Welcome to the first Triton puzzle! In this tutorial, we'll implement the simplest possible GPU kernel: element-wise vector addition.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to write a basic Triton kernel\n",
    "- The difference between HBM and SRAM in GPU programming\n",
    "- How to use Triton's pointer arithmetic and masking\n",
    "- How to benchmark GPU kernels against PyTorch\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "Vector addition is one of the simplest operations in linear algebra. Given two vectors $\\mathbf{x}$ and $\\mathbf{y}$ of length $n$:\n",
    "\n",
    "$$\\mathbf{x} = [x_1, x_2, ..., x_n]$$\n",
    "$$\\mathbf{y} = [y_1, y_2, ..., y_n]$$\n",
    "\n",
    "The element-wise addition is:\n",
    "\n",
    "$$\\mathbf{z} = \\mathbf{x} + \\mathbf{y} = [x_1 + y_1, x_2 + y_2, ..., x_n + y_n]$$\n",
    "\n",
    "### Memory Operations Analysis\n",
    "\n",
    "For vector addition, we need to:\n",
    "- **Read** $n$ elements from vector $\\mathbf{x}$ (from HBM to SRAM)\n",
    "- **Read** $n$ elements from vector $\\mathbf{y}$ (from HBM to SRAM)\n",
    "- **Compute** $n$ additions (on SRAM)\n",
    "- **Write** $n$ elements to vector $\\mathbf{z}$ (from SRAM to HBM)\n",
    "\n",
    "Cost:\n",
    "- Total memory operations: $3n$ (2 reads + 1 write)\n",
    "- Total FLOPs: $n$ (additions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced6d0c-17a5-4c87-9cbd-470941fec133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Set up device\n",
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f459ffb7-44b1-4fed-ae5c-abf9cb346cca",
   "metadata": {},
   "source": [
    "## Implementation 1: Raw PyTorch\n",
    "\n",
    "Let's start with the simplest implementation using PyTorch's built-in operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a451d-e792-4f59-afb5-712dfad5bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pytorch(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Element-wise vector addition using PyTorch.\"\"\"\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a4bd7-e424-4fe9-8b6d-ddade9f3f6c1",
   "metadata": {},
   "source": [
    "## Implementation 2: PyTorch with torch.compile\n",
    "\n",
    "PyTorch 2.0 introduced `torch.compile` which can optimize operations by fusing kernels and reducing memory traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f287db-03cc-40e9-a5b2-88ed9ca71938",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def add_pytorch_compiled(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Element-wise vector addition using torch.compile.\"\"\"\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1390fd-9ff7-4d95-a51d-61513129da65",
   "metadata": {},
   "source": [
    "## Implementation 3: Triton Kernel (Puzzle)\n",
    "\n",
    "Now for the main challenge! Implement vector addition using Triton.\n",
    "\n",
    "### Key Triton Concepts:\n",
    "\n",
    "1. **Programs and Blocks**: Triton launches multiple \"programs\" in parallel. Each program processes a block of data.\n",
    "2. **Pointers**: Triton uses pointer arithmetic similar to C/C++.\n",
    "3. **Masking**: To handle edge cases where data size isn't perfectly divisible by block size.\n",
    "4. **SRAM vs HBM**: Load data from slow HBM to fast SRAM, compute, then store back.\n",
    "\n",
    "### Your Task:\n",
    "Complete the Triton kernel below. You need to:\n",
    "1. Calculate which block this program should process\n",
    "2. Create offsets for memory access\n",
    "3. Load data from x and y\n",
    "4. Perform the addition\n",
    "5. Store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efaf181-ffba-4828-8187-8f55bd743ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def add_kernel(\n",
    "    x_ptr,  # Pointer to first input vector\n",
    "    y_ptr,  # Pointer to second input vector\n",
    "    output_ptr,  # Pointer to output vector\n",
    "    n_elements,  # Size of the vectors\n",
    "    BLOCK_SIZE: tl.constexpr,  # Number of elements each program processes\n",
    "):\n",
    "    \"\"\"\n",
    "    Triton kernel for element-wise vector addition.\n",
    "    Each program instance processes BLOCK_SIZE elements.\n",
    "    \"\"\"\n",
    "    # YOUR IMPLEMENTATION GOES HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2039f75-d9bf-45ea-979a-c0e0d0145d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for the Triton kernel\n",
    "def add_triton(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Wrapper function for the Triton vector addition kernel.\n",
    "    Handles output allocation and kernel launching.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are contiguous and on the correct device\n",
    "    x = x.contiguous()\n",
    "    y = y.contiguous()\n",
    "    \n",
    "    # Allocate output tensor\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    # Get the number of elements\n",
    "    n_elements = output.numel()\n",
    "    \n",
    "    # Define block size (must be power of 2)\n",
    "    BLOCK_SIZE = 1024  # we will see later how we can optimize this!\n",
    "    \n",
    "    # Calculate grid size (number of programs to launch)\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    \n",
    "    # Launch kernel\n",
    "    add_kernel[grid](\n",
    "        x, y, output, n_elements,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc914e8c-3d8e-457a-92a0-21da7ac65983",
   "metadata": {},
   "source": [
    "## Solution (Hidden)\n",
    "<font size=\"6\">üßô</font> You shall not pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b927455d-85d0-44e1-939c-83da5238a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def add_kernel(\n",
    "    x_ptr,\n",
    "    y_ptr,\n",
    "    output_ptr,\n",
    "    n_elements,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Triton kernel for element-wise vector addition.\n",
    "    Each program instance processes BLOCK_SIZE elements.\n",
    "    \"\"\"\n",
    "    # Identify which program we are\n",
    "    pid = tl.program_id(axis=0)\n",
    "    \n",
    "    # Calculate the starting index for this program\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    \n",
    "    # Create a range of offsets for this block\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Create a mask to guard against out-of-bounds accesses\n",
    "    mask = offsets < n_elements\n",
    "    \n",
    "    # Load x and y from DRAM to SRAM\n",
    "    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n",
    "    \n",
    "    # Perform the addition on SRAM\n",
    "    output = x + y\n",
    "    \n",
    "    # Write the result back to DRAM\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffaceba-30ab-4671-a137-ce6dcd5e949a",
   "metadata": {},
   "source": [
    "## FLOP Analysis\n",
    "\n",
    "Let's count the floating-point operations for each implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1865b-b19c-47df-a367-eb990e134469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_flops(n_elements):\n",
    "    \"\"\"\n",
    "    Count FLOPs for vector addition.\n",
    "    \n",
    "    Vector addition performs 1 addition per element.\n",
    "    \"\"\"\n",
    "    return n_elements\n",
    "\n",
    "def analyze_memory_ops(n_elements):\n",
    "    \"\"\"\n",
    "    Analyze memory operations for vector addition.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Memory operations breakdown\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'reads': 2 * n_elements,  # Read x and y\n",
    "        'writes': n_elements,     # Write output\n",
    "        'total': 3 * n_elements,  # Total memory ops\n",
    "        'arithmetic_intensity': 1/3  # FLOPs per memory op\n",
    "    }\n",
    "\n",
    "# Test FLOP counting\n",
    "n = 1_000_000\n",
    "flops = count_flops(n)\n",
    "mem_ops = analyze_memory_ops(n)\n",
    "\n",
    "print(f\"Vector addition for {n:,} elements:\")\n",
    "print(f\"  FLOPs: {flops:,}\")\n",
    "print(f\"  Memory reads: {mem_ops['reads']:,}\")\n",
    "print(f\"  Memory writes: {mem_ops['writes']:,}\")\n",
    "print(f\"  Total memory ops: {mem_ops['total']:,}\")\n",
    "print(f\"  Arithmetic intensity: {mem_ops['arithmetic_intensity']:.3f} FLOPs/byte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d0cf2-00a5-498c-8bd4-6e8d405b7927",
   "metadata": {},
   "source": [
    "## Testing Correctness\n",
    "\n",
    "Let's verify that our Triton implementation produces the same results as PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99009a99-39ea-4497-8094-568b66f4dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_correctness(size=1000000, atol=1e-5, rtol=1e-5):\n",
    "    \"\"\"Test if Triton implementation matches PyTorch.\"\"\"\n",
    "    # Create random input tensors\n",
    "    torch.manual_seed(42)\n",
    "    x = torch.randn(size, device=DEVICE, dtype=torch.float32)\n",
    "    y = torch.randn(size, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    # Compute with PyTorch\n",
    "    expected = add_pytorch(x, y)\n",
    "    \n",
    "    # Compute with Triton\n",
    "    actual = add_triton(x, y)\n",
    "    \n",
    "    # Check if results match\n",
    "    try:\n",
    "        torch.testing.assert_close(actual, expected, atol=atol, rtol=rtol)\n",
    "        print(f\"‚úÖ Test PASSED!\")\n",
    "        print(f\"   Max absolute difference: {(actual - expected).abs().max().item():.2e}\")\n",
    "        return True\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå Test FAILED!\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "test_passed = test_correctness()\n",
    "\n",
    "# Display congrats message\n",
    "if test_passed:\n",
    "    print(\"\\nüéâ Congratulations! Your implementation is correct!\")\n",
    "    display(Image(\"https://c.tenor.com/9d2wq28eb9UAAAAC/tenor.gif\", width=256, height=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cec899-0d6b-492e-8163-1b1b49cc8fd0",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "Now let's benchmark all three implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d45312-73a0-418a-a568-5e684d294a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['size'],  # Argument names to use as x-axis\n",
    "        x_vals=[2**i for i in range(12, 25)],  # Different sizes to benchmark\n",
    "        x_log=True,  # Log scale x-axis\n",
    "        line_arg='provider',  # Argument to separate lines\n",
    "        line_vals=['pytorch', 'compiled', 'triton'],  # Different implementations\n",
    "        line_names=['PyTorch', 'Torch Compiled', 'Triton'],  # Legend names\n",
    "        styles=[('blue', '-'), ('green', '--'), ('red', ':')],  # Line styles\n",
    "        ylabel='GB/s',  # Y-axis label\n",
    "        plot_name='vector-add-performance',  # Plot title\n",
    "        args={},  # Additional arguments\n",
    "    )\n",
    ")\n",
    "def benchmark(size, provider):\n",
    "    \"\"\"Benchmark vector addition implementations.\"\"\"\n",
    "    # Create input tensors\n",
    "    x = torch.rand(size, device=DEVICE, dtype=torch.float32)\n",
    "    y = torch.rand(size, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    # Define functions to benchmark\n",
    "    quantiles = [0.5, 0.05, 0.95]\n",
    "    \n",
    "    if provider == 'pytorch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add_pytorch(x, y), quantiles=quantiles)\n",
    "    elif provider == 'compiled':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add_pytorch_compiled(x, y), quantiles=quantiles)\n",
    "    elif provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add_triton(x, y), quantiles=quantiles)\n",
    "    \n",
    "    # Calculate GB/s: (2 reads + 1 write) * 4 bytes * size / time\n",
    "    gbps = lambda ms: 3 * x.element_size() * x.numel() * 1e-9 / (ms * 1e-3)\n",
    "    \n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "# Run benchmarks\n",
    "results = benchmark.run(print_data=True, return_df=True, save_path='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a4275-0574-4ecf-898d-3438c5a911d7",
   "metadata": {},
   "source": [
    "## Speedup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83fded-c688-44a2-9fb5-f319f83a7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Triton is faster than PyTorch\n",
    "avg_pytorch = results['PyTorch'].mean()\n",
    "avg_triton = results['Triton'].mean()\n",
    "speedup = avg_triton / avg_pytorch\n",
    "\n",
    "if speedup > 1.0:\n",
    "    print(f\"\\nüöÄ Awesome! Triton is {speedup:.2f}x faster than PyTorch!\")\n",
    "    display(Image(\"https://c.tenor.com/QFFzqAIAvnIAAAAd/tenor.gif\", width=400, height=256))\n",
    "else:\n",
    "    print(f\"\\nüêåüêåüêå Triton implementation is {speedup:.2f}x slower than PyTorch!. üêåüêåüêå\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45bcf65-03fb-43c7-9cc3-18786196e338",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. **Basic Triton concepts**: Programs, blocks, pointers, and masking\n",
    "2. **Memory hierarchy**: The importance of HBM vs SRAM in GPU programming\n",
    "3. **Performance analysis**: How to count FLOPs and memory operations\n",
    "4. **Benchmarking**: How to use Triton's built-in benchmarking tools\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- Vector addition is memory-bound (low arithmetic intensity of ~0.083 FLOPs/byte)\n",
    "- Triton can match or beat PyTorch performance even for simple operations\n",
    "- Block size affects performance - experiment with different values!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Ready for a harder challenge? Try the Fused Softmax puzzle next!\n",
    "\n",
    "<img src=\"sardine-challenge.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110de78-40f9-48c3-9429-f69aaa82954d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
