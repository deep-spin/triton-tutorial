{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9003910d-7f2e-4cb8-94af-1aaf6f4b5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_triton import setup_triton\n",
    "\n",
    "# TRITON_INTERPRET=1 uses a python interpreter instead of running on the GPU. \n",
    "# This menas that uou can insert Python breakpoints to debug your kernel code! \n",
    "setup_triton(use_interpreter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa1905-83c2-4f84-a6a1-b88e76da6076",
   "metadata": {},
   "source": [
    "# Triton Puzzle 2: Fused Softmax\n",
    "\n",
    "Welcome to the second Triton puzzle! Now we'll tackle a more complex operation: fused softmax. This builds on what you learned in vector addition while introducing new concepts.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to work with 2D data (matrices) in Triton\n",
    "- Performing reduction operations (max, sum) in SRAM\n",
    "- Why kernel fusion is crucial for performance\n",
    "- Introduction to `num_warps` for better parallelism\n",
    "\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "The softmax function transforms a vector of real numbers into a probability distribution. For a vector $\\mathbf{x} = [x_1, x_2, ..., x_n]$:\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$\n",
    "\n",
    "However, this naive formulation is numerically unstable for large values. The stable version subtracts the maximum:\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{e^{x_i - \\max(\\mathbf{x})}}{\\sum_{j=1}^{n} e^{x_j - \\max(\\mathbf{x})}}$$\n",
    "\n",
    "### Memory Operations Analysis\n",
    "\n",
    "#### Naive Implementation (4 kernels):\n",
    "1. Find max: Read $n$ ‚Üí Write $1$\n",
    "2. Subtract max: Read $n+1$ ‚Üí Write $n$\n",
    "3. Exponential: Read $n$ ‚Üí Write $n$\n",
    "4. Normalize: Read $n$ ‚Üí sum, divide ‚Üí Write $n$\n",
    "\n",
    "Total: 5 reads + 3 writes per element = **8 memory operations per element**\n",
    "\n",
    "#### Fused Implementation (1 kernel):\n",
    "1. Load once, compute everything in SRAM, store once\n",
    "\n",
    "Total: 1 read + 1 write per element = **2 memory operations per element**\n",
    "\n",
    "**4x reduction in memory traffic!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a2ceb-00f0-46c0-a3dc-e4bf5f21ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103d4ff-b149-41c2-ac9a-0f20a730adbe",
   "metadata": {},
   "source": [
    "## Implementation 1: Naive PyTorch (Unfused)\n",
    "\n",
    "Let's start with a naive implementation that clearly shows each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e4be3-36b4-46e2-9401-d045c3d31482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x):\n",
    "    \"\"\"\n",
    "    Naive softmax implementation - multiple kernel launches.\n",
    "    Input: (M, N) matrix\n",
    "    Output: (M, N) matrix with softmax applied row-wise\n",
    "    \"\"\"\n",
    "    # Step 1: Find maximum per row (first kernel)\n",
    "    x_max = x.max(dim=1, keepdim=True)[0]\n",
    "    \n",
    "    # Step 2: Subtract maximum for numerical stability (second kernel)\n",
    "    x_shifted = x - x_max\n",
    "    \n",
    "    # Step 3: Exponentiate (third kernel)\n",
    "    x_exp = torch.exp(x_shifted)\n",
    "    \n",
    "    # Step 4: Sum and normalize (fourth kernel)\n",
    "    x_sum = x_exp.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    return x_exp / x_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ccb8b2-d5ce-4fe4-81b6-3fe401cd3c7d",
   "metadata": {},
   "source": [
    "## Implementation 2: PyTorch Built-in\n",
    "\n",
    "PyTorch's built-in softmax is already optimized and fused:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e123de1-f23e-4b41-90e5-b3d02d5cfc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_pytorch(x):\n",
    "    \"\"\"PyTorch's built-in softmax - already fused.\"\"\"\n",
    "    return torch.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406aac14-2171-4aa1-b96d-663575a55446",
   "metadata": {},
   "source": [
    "## Implementation 3: PyTorch Compiled\n",
    "\n",
    "Let's see if torch.compile can fuse our naive implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ace02e-1941-4b59-aa70-64739bd82a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def softmax_compiled(x):\n",
    "    \"\"\"Compiled version of naive softmax.\"\"\"\n",
    "    x_max = x.max(dim=1, keepdim=True)[0]\n",
    "    x_shifted = x - x_max\n",
    "    x_exp = torch.exp(x_shifted)\n",
    "    x_sum = x_exp.sum(dim=1, keepdim=True)\n",
    "    return x_exp / x_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9134685f-a214-4d98-88c5-9309c4d68d8c",
   "metadata": {},
   "source": [
    "## Implementation 4: Triton Kernel (Puzzle)\n",
    "\n",
    "Now implement fused softmax in Triton! Here are the key concepts for this puzzle:\n",
    "\n",
    "### 1. Working with 2D Data\n",
    "- Each program processes one or more rows\n",
    "- We use `BLOCK_SIZE` to handle rows that don't fit in SRAM\n",
    "\n",
    "### 2. Reductions in Triton\n",
    "- `tl.max(x, axis=0)`: Find maximum along an axis\n",
    "- `tl.sum(x, axis=0)`: Sum along an axis\n",
    "- These operations happen entirely in fast SRAM!\n",
    "\n",
    "### 3. Program Organization\n",
    "- Each program handles one row of the matrix\n",
    "- Programs run in parallel across different SMs (Streaming Multiprocessors)\n",
    "\n",
    "### 4. Introduction to Warps\n",
    "- A **warp** is a group of 32 threads that execute in lockstep\n",
    "- `num_warps` controls how many warps are assigned to each program\n",
    "- More warps can help with parallelism but use more resources\n",
    "\n",
    "\n",
    "### Your Task:\n",
    "Complete the kernel to:\n",
    "1. Load a row of data (handling the case where row > BLOCK_SIZE)\n",
    "2. Find the maximum value in the row\n",
    "3. Compute exponentials with numerical stability\n",
    "4. Sum the exponentials\n",
    "5. Normalize and store the result\n",
    "\n",
    "Here's a video to help you out:\n",
    "\n",
    "<video width=\"700\" height=\"400\" src=\"figs/softmax_helper.mp4\" controls></video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5aaba-d86f-44c9-8871-e37a2da89a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "    input_ptr,      # Pointer to input matrix\n",
    "    output_ptr,     # Pointer to output matrix\n",
    "    n_cols,         # Number of columns (row size)\n",
    "    input_stride,   # Stride between rows in input\n",
    "    output_stride,  # Stride between rows in output\n",
    "    BLOCK_SIZE: tl.constexpr,  # Size of blocks to process\n",
    "):\n",
    "    \"\"\"\n",
    "    Fused softmax kernel. Each program computes one row.\n",
    "    \n",
    "    Key concepts:\n",
    "    - Each program gets a unique row index via tl.program_id(0)\n",
    "    - We process the row in chunks of BLOCK_SIZE\n",
    "    - All operations (max, exp, sum) happen in SRAM\n",
    "    \"\"\"\n",
    "    # Get the row index for this program\n",
    "    row_idx = tl.program_id(0)\n",
    "    \n",
    "    # Calculate the starting pointer for this row\n",
    "    input_row_start = input_ptr + row_idx * input_stride\n",
    "    output_row_start = output_ptr + row_idx * output_stride\n",
    "    \n",
    "    # YOUR IMPLEMENTATION GOES HERE\n",
    "    # Hints:\n",
    "    # 1. Process the row in BLOCK_SIZE chunks (use range)\n",
    "    # 2. Use tl.max() to find the maximum\n",
    "    # 3. Keep running sums for numerical stability\n",
    "    # 4. Remember to mask for rows not divisible by BLOCK_SIZE\n",
    "    pass\n",
    "\n",
    "\n",
    "def softmax_triton(x):\n",
    "    \"\"\"Wrapper for the Triton softmax kernel.\"\"\"\n",
    "    # Ensure input is contiguous\n",
    "    x = x.contiguous()\n",
    "    \n",
    "    # Get dimensions\n",
    "    n_rows, n_cols = x.shape\n",
    "    \n",
    "    # Allocate output\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    # BLOCK_SIZE must be power of 2 and >= n_cols\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "    \n",
    "    # Simple heuristic for num_warps:\n",
    "    # - More warps for larger blocks\n",
    "    # - Minimum 2, maximum 8\n",
    "    if BLOCK_SIZE >= 2048:\n",
    "        num_warps = 8\n",
    "    elif BLOCK_SIZE >= 1024:\n",
    "        num_warps = 4\n",
    "    else:\n",
    "        num_warps = 2\n",
    "    \n",
    "    # Launch grid: one program per row\n",
    "    grid = (n_rows,)\n",
    "    \n",
    "    # Launch kernel\n",
    "    softmax_kernel[grid](\n",
    "        x, output,\n",
    "        n_cols,\n",
    "        x.stride(0),  # Stride between rows\n",
    "        output.stride(0),\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "        num_warps=num_warps,\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a5ba2-79d5-4d8b-948a-328e5f56b7db",
   "metadata": {},
   "source": [
    "## Solution üßô\n",
    "\n",
    "You shall not pass! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d79c5-28f0-42a1-b1aa-ad2247a4727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our implementation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77519a1c-10bb-45c9-9e87-502f100149f9",
   "metadata": {},
   "source": [
    "## Testing Correctness\n",
    "\n",
    "Verify our implementation matches PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de08b6e-7fb6-40f1-a524-840bd91e8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_correctness(n_rows=100, n_cols=2048, atol=1e-5, rtol=1e-5):\n",
    "    \"\"\"Test if Triton implementation matches PyTorch.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    x = torch.randn(n_rows, n_cols, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    # Compute with PyTorch\n",
    "    expected = softmax_pytorch(x)\n",
    "    \n",
    "    # Compute with Triton\n",
    "    actual = softmax_triton(x)\n",
    "    \n",
    "    try:\n",
    "        torch.testing.assert_close(actual, expected, atol=atol, rtol=rtol)\n",
    "        print(f\"‚úÖ Test PASSED! Results match within tolerance.\")\n",
    "        print(f\"   Shape tested: ({n_rows}, {n_cols})\")\n",
    "        print(f\"   Max absolute difference: {(actual - expected).abs().max().item():.2e}\")\n",
    "        \n",
    "        # Test numerical stability with large values\n",
    "        x_large = torch.randn(10, 100, device=DEVICE) * 100\n",
    "        expected_large = softmax_pytorch(x_large)\n",
    "        actual_large = softmax_triton(x_large)\n",
    "        torch.testing.assert_close(actual_large, expected_large, atol=atol, rtol=rtol)\n",
    "        print(f\"‚úÖ Numerical stability test PASSED!\")\n",
    "        \n",
    "        return True\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå Test FAILED!\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run tests\n",
    "test_passed = test_correctness()\n",
    "\n",
    "# Display congrats message\n",
    "if test_passed:\n",
    "    print(\"\\nüéâ Congratulations! Your implementation is correct!\")\n",
    "    display(Image(\"figs/success.gif\", width=256, height=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e286a22e-4983-46b9-bfa4-081d4f461c2d",
   "metadata": {},
   "source": [
    "## FLOP and Memory Analysis\n",
    "\n",
    "Let's analyze the computational complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca9091-37cb-42f6-873b-50048dc02cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_softmax_ops(n_rows, n_cols):\n",
    "    \"\"\"Analyze operations for softmax computation.\"\"\"\n",
    "    \n",
    "    # FLOPs per row:\n",
    "    # - Finding max: n_cols comparisons\n",
    "    # - Subtraction: n_cols ops\n",
    "    # - Exp: n_cols ops (counted as multiple FLOPs)\n",
    "    # - Sum: n_cols additions\n",
    "    # - Division: n_cols ops\n",
    "    \n",
    "    exp_flops = 10  # Approximate FLOPs for exponential\n",
    "    \n",
    "    flops_per_row = (\n",
    "        n_cols +           # max\n",
    "        n_cols +           # subtract\n",
    "        n_cols * exp_flops + # exp\n",
    "        n_cols +           # sum\n",
    "        n_cols             # divide\n",
    "    )\n",
    "    \n",
    "    total_flops = n_rows * flops_per_row\n",
    "    \n",
    "    # Memory operations (in bytes)\n",
    "    element_size = 4  # float32\n",
    "    \n",
    "    naive_mem_ops = {\n",
    "        'reads': n_rows * n_cols * element_size * 4,   # Read 4 times\n",
    "        'writes': n_rows * n_cols * element_size * 3,  # Write 3 times\n",
    "        'total': n_rows * n_cols * element_size * 7\n",
    "    }\n",
    "    \n",
    "    fused_mem_ops = {\n",
    "        'reads': n_rows * n_cols * element_size * 1,   # Read once\n",
    "        'writes': n_rows * n_cols * element_size * 1,  # Write once\n",
    "        'total': n_rows * n_cols * element_size * 2\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'flops': total_flops,\n",
    "        'naive_memory': naive_mem_ops,\n",
    "        'fused_memory': fused_mem_ops,\n",
    "        'memory_reduction': naive_mem_ops['total'] / fused_mem_ops['total']\n",
    "    }\n",
    "\n",
    "# Example analysis\n",
    "n_rows, n_cols = 1024, 2048\n",
    "analysis = analyze_softmax_ops(n_rows, n_cols)\n",
    "\n",
    "print(f\"Softmax analysis for {n_rows}x{n_cols} matrix:\")\n",
    "print(f\"  Total FLOPs: {analysis['flops']:,}\")\n",
    "print(f\"  Naive memory traffic: {analysis['naive_memory']['total'] / 1e9:.2f} GB\")\n",
    "print(f\"  Fused memory traffic: {analysis['fused_memory']['total'] / 1e9:.2f} GB\")\n",
    "print(f\"  Memory reduction: {analysis['memory_reduction']:.1f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fced240-4a83-46b6-bbe5-11a13a4ffdd3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. **Kernel Fusion**: Why combining operations is crucial for performance\n",
    "2. **2D Operations**: How to work with matrices in Triton\n",
    "3. **Reductions**: Using `tl.max()` and `tl.sum()` for row-wise operations\n",
    "4. **num_warps**: Introduction to controlling thread parallelism\n",
    "5. **Numerical Stability**: Implementing stable softmax computation\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Memory Bandwidth**: Fused softmax uses 3.5x less memory bandwidth than naive\n",
    "- **SRAM Utilization**: All intermediate values stay in fast SRAM\n",
    "- **Block Processing**: Handling rows larger than SRAM capacity\n",
    "- **Parallelism**: Each row is processed independently by different programs\n",
    "\n",
    "### Performance Tips:\n",
    "\n",
    "- Adjust `BLOCK_SIZE` based on your typical row sizes\n",
    "- Experiment with `num_warps` for your specific GPU\n",
    "- Consider `num_stages` (next puzzle!) for even better performance\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Ready for matrix multiplication? The next puzzle introduces more advanced concepts like:\n",
    "- 2D block tiling\n",
    "- Shared memory optimization  \n",
    "- `num_stages` for pipelining\n",
    "- Auto-tuning for optimal performance\n",
    "\n",
    "\n",
    "<img src=\"figs/sardine-challenge.png\" width=\"512\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c55e688-e0fd-4741-94c5-8c5f8b4f0f43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmarking (GPUs only)\n",
    "\n",
    "Let's benchmark all implementations using Triton's tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ee49a-9033-47ec-8a8d-5bef24035a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['n_cols'],  # Column size as x-axis\n",
    "        x_vals=[128 * i for i in range(1, 33)],  # 128 to 4096\n",
    "        line_arg='provider',\n",
    "        line_vals=['naive', 'pytorch', 'compiled', 'triton'],\n",
    "        line_names=['Naive', 'PyTorch', 'Compiled', 'Triton'],\n",
    "        styles=[('blue', '-'), ('green', '--'), ('orange', '-.'), ('red', ':')],\n",
    "        ylabel='GB/s',\n",
    "        plot_name='softmax-performance',\n",
    "        args={'n_rows': 1024},  # Fix number of rows\n",
    "    )\n",
    ")\n",
    "def benchmark(n_rows, n_cols, provider):\n",
    "    \"\"\"Benchmark softmax implementations.\"\"\"\n",
    "    x = torch.randn(n_rows, n_cols, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    quantiles = [0.5, 0.05, 0.95]\n",
    "    \n",
    "    if provider == 'naive':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_naive(x), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'pytorch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_pytorch(x), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'compiled':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_compiled(x), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_triton(x), quantiles=quantiles\n",
    "        )\n",
    "    \n",
    "    # GB/s calculation for fused: 2 * size * 4 bytes / time\n",
    "    gbps = lambda ms: 2 * n_rows * n_cols * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    \n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"Running benchmarks...\")\n",
    "results = benchmark.run(print_data=True, return_df=True, save_path='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb0c7a9-75ad-44ef-9492-fb6601683a06",
   "metadata": {},
   "source": [
    "## Speedup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547d5cc-16b8-4868-9e9f-ec1a94d3e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Triton is faster than PyTorch\n",
    "avg_pytorch = results['PyTorch'].mean()\n",
    "avg_triton = results['Triton'].mean()\n",
    "speedup = avg_triton / avg_pytorch\n",
    "\n",
    "if speedup > 1.0:\n",
    "    print(f\"\\nüöÄ Awesome! Triton is {speedup:.2f}x faster than PyTorch!\")\n",
    "    display(Image(\"figs/gpu.gif\", width=400, height=256))\n",
    "else:\n",
    "    print(f\"\\nüêå Not quite yet! Triton implementation is {speedup:.2f}x slower than PyTorch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11569c8f-dfd8-4b43-8b65-f4f668595349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
