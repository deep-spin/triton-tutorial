{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a2b651-1e26-4509-b420-69e75ecc5c5c",
   "metadata": {},
   "source": [
    "# Triton Puzzle 4: Layer Normalization\n",
    "\n",
    "Welcome to the fourth Triton puzzle! Layer Normalization is a crucial component in modern deep learning, especially in transformers. This puzzle introduces parallel reduction patterns and online algorithms.\n",
    "\n",
    "### What you'll learn:\n",
    "- **Parallel reduction** patterns for computing statistics\n",
    "- **Welford's online algorithm** for numerically stable mean/variance\n",
    "- **Warp-level primitives** (`tl.reduce` with axis parameter)\n",
    "- Single-pass computation for efficiency\n",
    "- Numerical stability considerations\n",
    "- How to handle operations that don't map naturally to independent threads\n",
    "\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "Layer Normalization normalizes inputs across the feature dimension:\n",
    "\n",
    "Given input $\\mathbf{x} \\in \\mathbb{R}^{N \\times D}$ (batch size N, feature dimension D):\n",
    "\n",
    "$\\text{LayerNorm}(\\mathbf{x}_i) = \\gamma \\odot \\frac{\\mathbf{x}_i - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}} + \\beta$\n",
    "\n",
    "Where for each sample $i$:\n",
    "- $\\mu_i = \\frac{1}{D} \\sum_{j=1}^{D} x_{ij}$ (mean across features)\n",
    "- $\\sigma_i^2 = \\frac{1}{D} \\sum_{j=1}^{D} (x_{ij} - \\mu_i)^2$ (variance across features)\n",
    "- $\\gamma, \\beta \\in \\mathbb{R}^D$ are learned scale and shift parameters\n",
    "- $\\epsilon$ is a small constant for numerical stability\n",
    "\n",
    "\n",
    "### The Reduction Challenge\n",
    "\n",
    "Unlike our previous operations, LayerNorm requires **reduction** across the feature dimension:\n",
    "- Each thread needs information from ALL features to compute statistics\n",
    "- Can't process each element independently\n",
    "- Need efficient parallel reduction algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25f5ba-bf33-4e18-a111-7a8092974e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import numpy as np\n",
    "from IPython.display import IFrame, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0de40a-da8a-4528-b6b5-f9bf8cd5f335",
   "metadata": {},
   "source": [
    "## Implementation 1: Naive PyTorch (Two-pass)\n",
    "\n",
    "First, let's see a straightforward two-pass implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02799f-610f-4d64-9c14-1487da1bb71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layernorm_naive(x, weight, bias, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Naive two-pass implementation of LayerNorm.\n",
    "    First pass: compute mean\n",
    "    Second pass: compute variance\n",
    "    Third pass: normalize\n",
    "    \"\"\"\n",
    "    # Assume x is (N, D) where we normalize over D\n",
    "    mean = x.mean(dim=-1, keepdim=True)  # (N, 1)\n",
    "    var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)  # (N, 1)\n",
    "    std = torch.sqrt(var + eps)\n",
    "    # Normalize\n",
    "    x_norm = (x - mean) / std\n",
    "    # Scale and shift\n",
    "    return weight * x_norm + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a9d98-f3c1-4e7e-b97f-c048bf857bb8",
   "metadata": {},
   "source": [
    "## Implementation 2: PyTorch Built-in\n",
    "\n",
    "PyTorch's built-in uses optimized CUDA kernels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec04249-5be4-4cc5-a47a-0ce84f05907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layernorm_pytorch(x, weight, bias, eps=1e-5):\n",
    "    \"\"\"PyTorch's built-in LayerNorm.\"\"\"\n",
    "    return torch.nn.functional.layer_norm(x, x.shape[-1:], weight, bias, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6a240-51a3-4b6a-838b-b174cb09e392",
   "metadata": {},
   "source": [
    "## Implementation 3: PyTorch Compiled\n",
    "\n",
    "Let's try to compile a naive version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e701a775-0cf2-4902-a003-31a757112fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def layernorm_compiled(x, weight, bias, eps=1e-5):\n",
    "    mean = x.mean(dim=-1, keepdim=True)  # (N, 1)\n",
    "    var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)  # (N, 1)\n",
    "    std = torch.sqrt(var + eps)\n",
    "    x_norm = (x - mean) / std\n",
    "    return weight * x_norm + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add58259-5750-4fd7-83fb-1256d8f98129",
   "metadata": {},
   "source": [
    "## Key Concepts for This Puzzle\n",
    "\n",
    "### 1. Parallel Reduction in Triton\n",
    "\n",
    "Triton provides `tl.reduce` for efficient reduction operations:\n",
    "\n",
    "```python\n",
    "# Sum reduction along axis 0\n",
    "sum_val = tl.sum(data, axis=0)\n",
    "\n",
    "# You can also use tl.reduce with custom operations\n",
    "mean_val = tl.sum(data, axis=0) / num_elements\n",
    "```\n",
    "\n",
    "### 2. Single-Pass vs Multi-Pass Algorithms\n",
    "\n",
    "**Multi-pass (naive)**:\n",
    "- Pass 1: Compute mean\n",
    "- Pass 2: Compute variance using mean\n",
    "- Pass 3: Normalize\n",
    "\n",
    "**Single-pass (efficient)**:\n",
    "- Compute mean and variance in one pass\n",
    "- Use online algorithms for numerical stability\n",
    "\n",
    "### 3. Welford's Online Algorithm\n",
    "\n",
    "For numerical stability, we can compute variance using Welford's algorithm:\n",
    "\n",
    "```python\n",
    "# Instead of: var = mean((x - mean)^2)\n",
    "# Use incremental updates that are numerically stable\n",
    "```\n",
    "\n",
    "### 4. Block Size Considerations\n",
    "\n",
    "- Each program handles one sample (row)\n",
    "- Block size should be large enough to process all features\n",
    "- Need to handle cases where D > BLOCK_SIZE\n",
    "\n",
    "### 5. Memory Access Pattern\n",
    "\n",
    "Unlike previous puzzles:\n",
    "- Each program needs to read ALL features for its sample\n",
    "- Output has the same shape as input\n",
    "- Need weight and bias vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e84fbdb-b395-43f8-9460-212709c3187f",
   "metadata": {},
   "source": [
    "## Implementation 4: Triton Kernel (Puzzle)\n",
    "\n",
    "Now implement LayerNorm in Triton!\n",
    "\n",
    "### Your Task:\n",
    "1. Each program handles one sample (row)\n",
    "2. Load features in blocks\n",
    "3. Compute mean using `tl.sum`\n",
    "4. Compute variance (in the same pass if possible)\n",
    "5. Normalize and apply scale/shift\n",
    "6. Handle cases where feature dimension > BLOCK_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4785306-11f2-45c0-82fe-933a80080c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def layernorm_kernel(\n",
    "    x_ptr, y_ptr, weight_ptr, bias_ptr,\n",
    "    N, D,  # N = batch size, D = feature dimension\n",
    "    eps: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    \"\"\"\n",
    "    LayerNorm kernel where each program handles one sample.\n",
    "    \n",
    "    Key challenges:\n",
    "    - Compute mean and variance across D dimension\n",
    "    - Handle D > BLOCK_SIZE by looping\n",
    "    - Maintain numerical stability\n",
    "    \"\"\"\n",
    "    # YOUR IMPLEMENTATION GOES HERE\n",
    "    # Hints:\n",
    "    # 1. Use tl.program_id(0) to get which sample this program handles\n",
    "    # 2. Loop over features in blocks of BLOCK_SIZE\n",
    "    # 3. Accumulate sum and sum of squares for mean/variance\n",
    "    # 4. After computing statistics, loop again to normalize\n",
    "    # 5. Don't forget to apply weight and bias!\n",
    "    pass\n",
    "\n",
    "def layernorm_triton(x, weight, bias, eps=1e-5):\n",
    "    \"\"\"Wrapper for the Triton LayerNorm kernel.\"\"\"\n",
    "    assert x.is_contiguous()\n",
    "    assert weight.is_contiguous() \n",
    "    assert bias.is_contiguous()\n",
    "    \n",
    "    N, D = x.shape\n",
    "    \n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "    \n",
    "    # Choose block size (must be power of 2)\n",
    "    BLOCK_SIZE = triton.next_power_of_2(min(D, 1024))\n",
    "    \n",
    "    # Launch grid: one program per sample\n",
    "    grid = (N,)\n",
    "    \n",
    "    # Launch kernel\n",
    "    layernorm_kernel[grid](\n",
    "        x, y, weight, bias,\n",
    "        N, D,\n",
    "        eps,\n",
    "        BLOCK_SIZE\n",
    "    )\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cd4f6-4c6d-442f-b335-f7c3fe0e8521",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Solution (🧙 You shall not pass!)\n",
    "\n",
    "Here is a simple solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5b132-fe67-49df-bdd1-b658fc36f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def layernorm_kernel(\n",
    "    x_ptr, y_ptr, weight_ptr, bias_ptr,\n",
    "    N, D,\n",
    "    eps: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    \"\"\"LayerNorm kernel - each program handles one sample.\"\"\"\n",
    "    # Program ID = which sample (row) we're processing\n",
    "    row = tl.program_id(0)\n",
    "    \n",
    "    # Base pointers for this row\n",
    "    x_row_ptr = x_ptr + row * D\n",
    "    y_row_ptr = y_ptr + row * D\n",
    "    \n",
    "    # First pass: compute mean and variance\n",
    "    # We'll accumulate in chunks of BLOCK_SIZE\n",
    "    mean = 0.0\n",
    "    var = 0.0\n",
    "    \n",
    "    for start_idx in range(0, D, BLOCK_SIZE):\n",
    "        # Create offset mask\n",
    "        offs = start_idx + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offs < D\n",
    "        \n",
    "        # Load chunk of input\n",
    "        x = tl.load(x_row_ptr + offs, mask=mask, other=0.0)\n",
    "        \n",
    "        # Accumulate sum for mean\n",
    "        mean += tl.sum(x, axis=0)\n",
    "    \n",
    "    # Compute mean\n",
    "    mean = mean / D\n",
    "    \n",
    "    # Second pass: compute variance\n",
    "    for start_idx in range(0, D, BLOCK_SIZE):\n",
    "        offs = start_idx + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offs < D\n",
    "        \n",
    "        # Load chunk\n",
    "        x = tl.load(x_row_ptr + offs, mask=mask, other=0.0)\n",
    "        \n",
    "        # Accumulate variance\n",
    "        diff = x - mean\n",
    "        var += tl.sum(diff * diff, axis=0)\n",
    "    \n",
    "    # Compute variance and standard deviation\n",
    "    var = var / D\n",
    "    std = tl.sqrt(var + eps)\n",
    "    \n",
    "    # Third pass: normalize and write output\n",
    "    for start_idx in range(0, D, BLOCK_SIZE):\n",
    "        offs = start_idx + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offs < D\n",
    "        \n",
    "        # Load input, weight, and bias\n",
    "        x = tl.load(x_row_ptr + offs, mask=mask, other=0.0)\n",
    "        w = tl.load(weight_ptr + offs, mask=mask, other=1.0)\n",
    "        b = tl.load(bias_ptr + offs, mask=mask, other=0.0)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / std\n",
    "        \n",
    "        # Scale and shift\n",
    "        y = w * x_norm + b\n",
    "        \n",
    "        # Store output\n",
    "        tl.store(y_row_ptr + offs, y, mask=mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1e0d8-5e2e-4a82-830b-a9a99e4f93ed",
   "metadata": {},
   "source": [
    "### Advanced Single-Pass Solution\n",
    "\n",
    "For better performance, there are more efficient algorithms (Welford's algorithm), which computes mean and variance in a single-pass. But, we won't cover here because it is out of the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1395dc6-b84d-4a7a-a0fd-34cefd1d27a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ea72fe2-f607-42ae-b29c-30a4560aeb03",
   "metadata": {},
   "source": [
    "## FLOP and Memory Analysis\n",
    "\n",
    "Matrix multiplication has much higher arithmetic intensity than our previous operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc668c-7b28-4281-a243-4407bcfafdf2",
   "metadata": {},
   "source": [
    "## Testing Correctness\n",
    "\n",
    "Let's verify our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9732b-fe80-4682-ac7d-eb2c7d226896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_correctness(N=32, D=256, eps=1e-5, atol=1e-3, rtol=1e-3):\n",
    "    \"\"\"Test if Triton implementation matches PyTorch.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Create test inputs\n",
    "    x = torch.randn(N, D, device=DEVICE, dtype=torch.float32)\n",
    "    weight = torch.randn(D, device=DEVICE, dtype=torch.float32)\n",
    "    bias = torch.randn(D, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    # Compute with PyTorch\n",
    "    expected = layernorm_pytorch(x, weight, bias, eps)\n",
    "    \n",
    "    # Compute with Triton\n",
    "    actual = layernorm_triton(x, weight, bias, eps)\n",
    "    \n",
    "    try:\n",
    "        torch.testing.assert_close(actual, expected, atol=atol, rtol=rtol)\n",
    "        print(f\"✅ Test PASSED! Results match within tolerance.\")\n",
    "        print(f\"   Shape: ({N}, {D})\")\n",
    "        print(f\"   Max absolute difference: {(actual - expected).abs().max().item():.2e}\")\n",
    "        \n",
    "        # Test edge cases\n",
    "        test_cases = [\n",
    "            (1, 1024),    # Single sample\n",
    "            (128, 64),    # Small features\n",
    "            (64, 2048),   # Large features\n",
    "        ]\n",
    "        \n",
    "        for n, d in test_cases:\n",
    "            x_test = torch.randn(n, d, device=DEVICE, dtype=torch.float32)\n",
    "            w_test = torch.randn(d, device=DEVICE, dtype=torch.float32)\n",
    "            b_test = torch.randn(d, device=DEVICE, dtype=torch.float32)\n",
    "            \n",
    "            expected_test = layernorm_pytorch(x_test, w_test, b_test, eps)\n",
    "            actual_test = layernorm_triton(x_test, w_test, b_test, eps)\n",
    "            \n",
    "            torch.testing.assert_close(actual_test, expected_test, atol=atol, rtol=rtol)\n",
    "            print(f\"✅ Size ({n}, {d}) PASSED!\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        print(f\"❌ Test FAILED!\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run tests\n",
    "test_passed = test_correctness()\n",
    "\n",
    "# Display congrats message\n",
    "if test_passed:\n",
    "    print(\"\\n🎉 Congratulations! Your implementation is correct!\")\n",
    "    display(Image(\"https://c.tenor.com/9d2wq28eb9UAAAAC/tenor.gif\", width=256, height=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931abea-55e4-4a94-a8a3-3635ea58924a",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "Now let's benchmark the implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca8820-12ec-4224-a0b5-725dfe4a75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['D'],  # Feature dimension\n",
    "        x_vals=[128, 256, 512, 768, 1024, 2048, 4096],\n",
    "        line_arg='provider',\n",
    "        line_vals=['pytorch', 'compile', 'triton'],\n",
    "        line_names=['PyTorch', 'torch.compile', 'Triton'],\n",
    "        styles=[('green', '-'), ('red', '--'), ('blue', '-.')],\n",
    "        ylabel='GB/s',\n",
    "        plot_name='layernorm-performance',\n",
    "        args={'N': 1024},  # Batch size\n",
    "    )\n",
    ")\n",
    "def benchmark(D, N, provider):\n",
    "    \"\"\"Benchmark LayerNorm.\"\"\"\n",
    "    x = torch.randn(N, D, device=DEVICE, dtype=torch.float32)\n",
    "    weight = torch.randn(D, device=DEVICE, dtype=torch.float32)\n",
    "    bias = torch.randn(D, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    quantiles = [0.5, 0.05, 0.95]\n",
    "    \n",
    "    if provider == 'pytorch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: layernorm_pytorch(x, weight, bias), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: layernorm_compiled(x, weight, bias), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: layernorm_triton(x, weight, bias), quantiles=quantiles\n",
    "        )\n",
    "    \n",
    "    # Calculate bandwidth\n",
    "    bytes_moved = x.numel() * x.element_size() * 2  # Read + write\n",
    "    bytes_moved += weight.numel() * weight.element_size() * N  # Weight read N times\n",
    "    bytes_moved += bias.numel() * bias.element_size() * N  # Bias read N times\n",
    "    \n",
    "    gb_per_s = lambda ms: bytes_moved / ms / 1e6\n",
    "    \n",
    "    return gb_per_s(ms), gb_per_s(max_ms), gb_per_s(min_ms)\n",
    "\n",
    "print(\"Running benchmarks...\")\n",
    "results = benchmark.run(show_plots=True, print_data=True, return_df=True, save_path='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832974a-f51e-4769-8238-e8a04b62719b",
   "metadata": {},
   "source": [
    "## Speedup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ed33c-48fe-4bf0-b6f8-6392eb5bd290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Triton is faster than PyTorch\n",
    "avg_pytorch = results['PyTorch'].mean()\n",
    "avg_triton = results['Triton'].mean()\n",
    "speedup = avg_triton / avg_pytorch\n",
    "\n",
    "if speedup > 1.0:\n",
    "    print(f\"\\n🚀 Awesome! Triton is {speedup:.2f}x faster than PyTorch!\")\n",
    "    display(Image(\"https://c.tenor.com/QFFzqAIAvnIAAAAd/tenor.gif\", width=400, height=256))\n",
    "else:\n",
    "    print(f\"\\n🐌🐌🐌 Triton implementation is {speedup:.2f}x slower than PyTorch!. 🐌🐌🐌\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e507a-6cd2-46b6-a246-c88d925949bc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully implemented LayerNorm, mastering several advanced concepts!\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "\n",
    "1. **Parallel Reduction**: Computing statistics across a dimension\n",
    "2. **Multi-pass vs Single-pass**: Trade-offs between simplicity and efficiency\n",
    "3. **Numerical Stability**: Handling variance computation carefully\n",
    "4. **Variable-length Processing**: Using masks for features > BLOCK_SIZE\n",
    "5. **Memory Efficiency**: Minimizing passes over data\n",
    "\n",
    "\n",
    "### Performance Insights:\n",
    "\n",
    "- **Bandwidth-bound**: LayerNorm is limited by memory bandwidth, not compute\n",
    "- **Single-pass wins**: Reducing memory traffic is crucial\n",
    "- **Block processing**: Handle arbitrary feature dimensions efficiently\n",
    "- **Coalesced access**: Each program processes contiguous features\n",
    "\n",
    "\n",
    "### Implementation Tips:\n",
    "\n",
    "- Start with multi-pass for correctness, optimize to single-pass\n",
    "- Use appropriate block sizes (powers of 2, fitting in SRAM)\n",
    "- Consider Welford's algorithm for numerical stability\n",
    "- Remember that each program handles one sample\n",
    "\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Ready for CrossEntropy Loss? The final puzzle introduces:\n",
    "- Fusing the output layer\n",
    "- Dealing with high dimensionality\n",
    "- Integrating Triton & PyTorch\n",
    "\n",
    "Let's continue with our Triton journey!\n",
    "\n",
    "<img src=\"sardine-challenge.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958c4a9-e81b-4bdc-92ad-15d523d0a24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
