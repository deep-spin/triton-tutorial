{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1345d-d765-4f98-9024-0b32af792cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_triton import setup_triton\n",
    "\n",
    "# TRITON_INTERPRET=1 uses a python interpreter instead of running on the GPU. \n",
    "# This menas that uou can insert Python breakpoints to debug your kernel code! \n",
    "setup_triton(use_interpreter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9804c88-d4ff-45c6-9eac-e1413085db2c",
   "metadata": {},
   "source": [
    "# Triton Puzzle 4: Matrix Multiplication\n",
    "\n",
    "Welcome to the fourth Triton puzzle! Matrix multiplication is a fundamental operation in deep learning and a perfect example to learn advanced GPU optimization techniques.\n",
    "\n",
    "### What you'll learn:\n",
    "- 2D block tiling for data reuse\n",
    "- Working with 2D program grids\n",
    "- Introduction to `num_stages` for software pipelining\n",
    "- **Auto-tuning**: Automatically finding optimal parameters\n",
    "- Accumulation patterns in SRAM\n",
    "- Why matmul achieves much higher arithmetic intensity than previous operations\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "Matrix multiplication computes $\\mathbf{C} = \\mathbf{A} \\times \\mathbf{B}$ where:\n",
    "- $\\mathbf{A}$ is an $(M, K)$ matrix\n",
    "- $\\mathbf{B}$ is a $(K, N)$ matrix  \n",
    "- $\\mathbf{C}$ is an $(M, N)$ matrix\n",
    "\n",
    "Each element is computed as:\n",
    "\n",
    "$C_{ij} = \\sum_{k=1}^{K} A_{ik} \\cdot B_{kj}$\n",
    "\n",
    "\n",
    "### Naive Strategy\n",
    "\n",
    "The simplest method for matrix multiplication is to iterate over rows and columns:\n",
    "\n",
    "```python\n",
    "for m in range(0, M):\n",
    "    for n in range(0, N):\n",
    "        for k in range(0, K):\n",
    "            # Load element A_{m,k}\n",
    "            a = A[m, k]\n",
    "            # Load element B_{k,n}\n",
    "            b = B[k, n]\n",
    "            # Accumulate and store direct into C_{m, n}\n",
    "            C[m, n] += a * b\n",
    "```\n",
    "\n",
    "- **Total FLOPs:** 2MNK due to one multiply + one add per (m, n, k) triplet\n",
    "- **Memory traffic:**\n",
    "    - A reads: $MKN$\n",
    "    - B reads: $MKN$\n",
    "    - C writes: $MN$\n",
    "    - **Total:** $2MKN + MN$\n",
    "\n",
    "Can we do better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb97c9-eb10-45cc-96c2-2283a03e3056",
   "metadata": {},
   "source": [
    "### The Tiling Strategy\n",
    "\n",
    "Since matrices often don't fit in SRAM, we use **block tiling**. This image helps to explain the process:\n",
    "\n",
    "<img src=\"figs/tiled-outer-prod.svg\" width=\"512\" />\n",
    "\n",
    "Pseudocode:\n",
    "\n",
    "```python\n",
    "# B_M  = BLOCK_SIZE_M (tile size for A' rows)\n",
    "# B_N  = BLOCK_SIZE_N (tile size for B' cols)\n",
    "# B_K  = BLOCK_SIZE_K (tile size for inner dim)\n",
    "\n",
    "for m in range(0, M, B_M):\n",
    "    for n in range(0, N, B_N):\n",
    "        # Initialize accumulator for C in SRAM\n",
    "        acc = zeros((B_M, B_N))\n",
    "        for k in range(0, K, B_K):\n",
    "            # Load blocks into SRAM\n",
    "            a = A[m:m+B_M, k:k+B_K]\n",
    "            b = B[k:k+B_K, n:n+B_N]\n",
    "            # Accumulate in SRAM\n",
    "            acc += a @ b\n",
    "        # Store result\n",
    "        C[m:m+B_M, n:n+B_N] = acc\n",
    "```\n",
    "\n",
    "- **Total FLOPs:** still 2MNK (same computation, different order)\n",
    "- **Memory traffic:**\n",
    "  - A reads: $MK \\times (N/B_N) = MKN/B_N$\n",
    "  - B reads: $KN \\times (M/B_M) = MKN/B_M$\n",
    "  - C writes: $MN$\n",
    "  - **Total:** $MKN(1/B_N + 1/B_M) + MN$\n",
    "\n",
    "### Comparison\n",
    "\n",
    "For $B_M = B_N = B$, we have:\n",
    "\n",
    "- **Naive approach:** $2MKN + MN$ bytes of memory traffic\n",
    "- **Tiled approach:** $2MKN/B + MN$ bytes of memory traffic\n",
    "- **Memory traffic reduction:** $\\approx B\\times$ for large $K$\n",
    "\n",
    "<font color=\"green\"><b>\n",
    "    This is why tiling is so powerful! We've reduced memory traffic by the block size factor while doing exactly the same computation.</b></font> \n",
    "\n",
    "\n",
    "### Key Insight: Arithmetic Intensity\n",
    "\n",
    "$\\text{Arithmetic Intensity} = \\dfrac{\\text{FLOPs}}{\\text{moved bytes}}$\n",
    "\n",
    "- **Naive approach:** $2MNK / (2MNK + MN) \\approx 1 \\text{ FLOP/byte }$\n",
    "- **Tiled approach (B=128):** $2MNK / (2MNK/128 + MN) \\approx 128 \\, \\text{ FLOPs/byte }$\n",
    "\n",
    "<font color=\"green\"><b>\n",
    "    This massive increase in arithmetic intensity is what makes matrix multiplication so efficient on GPUs!\n",
    "</b></font> \n",
    "\n",
    "<font color=\"red\"><b>\n",
    "    As long as our blocks fit in SRAM, we are good to go!\n",
    "</b></font>\n",
    "\n",
    "So, let's code a matmul in Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b599d-c64e-4a93-9288-440bcb1dec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc68c44-fe48-4919-a28d-6053fcac8bb3",
   "metadata": {},
   "source": [
    "## Implementation 1: PyTorch Built-in\n",
    "\n",
    "First, let's see PyTorch's built-in implementation, which uses highly optimized cuda kernels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563541e-3a29-4136-8dda-ac80da554246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_pytorch(A, B):\n",
    "    \"\"\"PyTorch's built-in matrix multiplication.\"\"\"\n",
    "    return torch.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655fc5c-cf5b-4063-aff8-c1cf2282d1a6",
   "metadata": {},
   "source": [
    "## Implementation 2: PyTorch Compiled\n",
    "\n",
    "Let's see a naive triply-nested loop implementation optimized with torch.compile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e493c-03ff-43ab-8ca9-78efc899cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def matmul_compiled(A, B, BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32):\n",
    "    M, K = A.shape\n",
    "    _, N = B.shape\n",
    "    C = torch.zeros((M, N), dtype=A.dtype, device=A.device)\n",
    "    for m in range(0, M, BLOCK_SIZE_M):\n",
    "        for n in range(0, N, BLOCK_SIZE_N):\n",
    "            # no accumulator for simplicity\n",
    "            for k in range(0, K, BLOCK_SIZE_K):\n",
    "                a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\n",
    "                b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\n",
    "                C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] += a @ b\n",
    "    return C\n",
    "\n",
    "\n",
    "# Warmup & test correctness\n",
    "# A = torch.randn(128, 128)\n",
    "# B = torch.randn(128, 128)\n",
    "# C_ref = matmul_pytorch(A, B)\n",
    "# C_comp = matmul_compiled(A, B)\n",
    "# torch.allclose(C_ref, C_comp, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522c70a-674f-41b6-a296-beb246d07700",
   "metadata": {},
   "source": [
    "## Implementation 3: Triton Kernel (Puzzle)\n",
    "\n",
    "Key concepts for this puzzle:\n",
    "\n",
    "### 1. 2D Program Grids\n",
    "- Programs are organized in a 2D grid: `(M/BLOCK_SIZE_M, N/BLOCK_SIZE_N)`\n",
    "- Each program computes one output block\n",
    "- Use `tl.program_id(0)` and `tl.program_id(1)` for row and column indices, respectively\n",
    "\n",
    "### 2. Accumulation Pattern\n",
    "- Initialize accumulator in SRAM: `acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)`\n",
    "- Loop over K dimension, accumulating partial results\n",
    "- Only write to HBM once at the end\n",
    "\n",
    "### 3. Software Pipelining with `num_stages`\n",
    "- **num_stages** controls how many iterations of a loop can be \"parallelized\"\n",
    "    - I don't excatly know how it works, but usually num_stages > 1 improves performance\n",
    "    - That is, higher values allow some \"overlapping memory loads\", which helps computation\n",
    "    - Example: While computing on data from iteration `i`, we can load data for iteration `i+1`\n",
    "\n",
    "### 4. Auto-tuning Introduction\n",
    "- Different matrix sizes benefit from different block sizes\n",
    "- Triton can automatically test multiple configurations\n",
    "- We'll use `@triton.autotune` decorator\n",
    "\n",
    "\n",
    "Now implement matrix multiplication in Triton!\n",
    "\n",
    "### Your Task:\n",
    "1. Set up 2D program indices\n",
    "2. Initialize accumulator\n",
    "3. Loop over K dimension in blocks\n",
    "4. Load blocks of A and B\n",
    "5. Accumulate using `tl.dot`\n",
    "6. Store final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab96208-a32c-4e19-962e-390e74d29b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define configurations for auto-tuning\n",
    "# These values are largely empirical and many people usually just copy from each other\n",
    "# A rule of thumb is to ALWAYS use values that are a power of 2\n",
    "configs = [\n",
    "    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=2, num_warps=8),\n",
    "    triton.Config({'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4),\n",
    "    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "    triton.Config({'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "]\n",
    "@triton.autotune(\n",
    "    configs=configs,\n",
    "    key=['M', 'N', 'K'],  # Autotune based on matrix dimensions \n",
    "                          # when these values change, Triton will tune the tl.constexpr args!\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    # Pointers to matrices\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    # Matrix dimensions\n",
    "    M, N, K,\n",
    "    # Strides\n",
    "    stride_am, stride_ak,  # A matrix strides\n",
    "    stride_bk, stride_bn,  # B matrix strides  \n",
    "    stride_cm, stride_cn,  # C matrix strides\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_M: tl.constexpr, \n",
    "    BLOCK_SIZE_N: tl.constexpr, \n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Kernel for computing C = A @ B.\n",
    "    \n",
    "    Each program computes a [BLOCK_SIZE_M, BLOCK_SIZE_N] block of C.\n",
    "    \n",
    "    Key concepts:\n",
    "    - 2D program grid: programs are indexed by (pid_m, pid_n)\n",
    "    - Accumulation: partial results stay in SRAM\n",
    "    - Tiling: process K dimension in chunks of BLOCK_SIZE_K\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR IMPLEMENTATION GOES HERE.\n",
    "    # (We left you some helpful code here to kick it off)\n",
    "    \n",
    "    # Get program IDs for M and N dimensions\n",
    "    pid_m = tl.program_id(0)  # Row block index\n",
    "    pid_n = tl.program_id(1)  # Column block index\n",
    "    \n",
    "    # Set up offsets for this block\n",
    "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    \n",
    "    # Initialize accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "\n",
    "    # Initialize pointers to blocks of A and B\n",
    "    # todo\n",
    "    \n",
    "    # Loop over K dimension\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load A and B blocks, multiply, accumulate\n",
    "        # todo\n",
    "        \n",
    "        # Use tl.dot() for matrix multiplication\n",
    "        # todo\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    # Store final result with appropriate masking\n",
    "    # todo\n",
    "    pass\n",
    "\n",
    "\n",
    "def matmul_triton(a, b):\n",
    "    \"\"\"Wrapper for the Triton matmul kernel.\"\"\"\n",
    "    # Check dimensions\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n",
    "    \n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    \n",
    "    # Allocate output\n",
    "    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n",
    "    \n",
    "    # Create 2D launch grid\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(M, META['BLOCK_SIZE_M']),\n",
    "        triton.cdiv(N, META['BLOCK_SIZE_N']),\n",
    "    )\n",
    "    \n",
    "    # Launch kernel\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "    )\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c2ce64-095f-42b4-81ef-4ef210903adc",
   "metadata": {},
   "source": [
    "## Understanding Auto-tuning\n",
    "\n",
    "Let's see how auto-tuning selects the best configuration:\n",
    "\n",
    "> If the environment variable TRITON_PRINT_AUTOTUNING is set to \"1\", <br>\n",
    "> Triton will print a message to stdout after autotuning each kernel, <br>\n",
    "> including the time spent autotuning and the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3dc662-1fcb-4b51-a5cf-c3ee432137ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_autotune_info():\n",
    "    \"\"\"Display information about auto-tuning.\"\"\"\n",
    "    print(\"Auto-tuning configurations:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Config':<10} {'BLOCK_M':<10} {'BLOCK_N':<10} {'BLOCK_K':<10} {'num_warps':<10} {'num_stages':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, config in enumerate(configs):\n",
    "        block_m = config.kwargs['BLOCK_SIZE_M']\n",
    "        block_n = config.kwargs['BLOCK_SIZE_N']\n",
    "        block_k = config.kwargs['BLOCK_SIZE_K']\n",
    "        warps = config.num_warps\n",
    "        stages = config.num_stages\n",
    "        \n",
    "        print(f\"{i:<10} {block_m:<10} {block_n:<10} {block_k:<10} {warps:<10} {stages:<10}\")\n",
    "    \n",
    "    print(\"\\nAuto-tuning will test each configuration and select the fastest!\")\n",
    "    print(\"The best config depends on:\")\n",
    "    print(\"  - Matrix dimensions (M, N, K)\")\n",
    "    print(\"  - GPU architecture\")\n",
    "    print(\"  - Speed (default `bench` returns ms)\")\n",
    "\n",
    "show_autotune_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c13a0-37a5-4df9-9a9b-86d1bc3f1eb4",
   "metadata": {},
   "source": [
    "## Solution(s) 🧙\n",
    "\n",
    "You shall not pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08544e09-ce15-4dc9-ac75-5222e5fd8679",
   "metadata": {},
   "source": [
    "### Standard Solution\n",
    "\n",
    "Use a 2D grid and recover blocks with `tl.program_id(axis=0)` and `tl.program_id(axis=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407a415-3e50-4ef2-adf4-e6df409fa1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a03d03-03d2-446c-ac8e-520ad5fc2da3",
   "metadata": {},
   "source": [
    "### L2 Cache Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9afe93-1987-4099-9ce2-2823a9c7ba33",
   "metadata": {},
   "source": [
    "As per the original tutorial: https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html\n",
    "\n",
    "If we want to maximize L2 cache hit rate, we need to move from a simple row-major ordering to a grou-based ordering. \n",
    "This assumes you have a single axis of parallelism (rows) and thus you need to be smart in how you selected the columns.\n",
    "This can be done by ‘super-grouping’ blocks in groups of GROUP_M rows before switching to the next column.\n",
    "\n",
    "> For example, in the following matmul where each matrix is 9 blocks by 9 blocks, we can see that if we compute the output in row-major ordering, we need to load 90 blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped ordering, we only need to load 54 blocks.\n",
    "\n",
    "<img src=\"https://triton-lang.org/main/_images/grouped_vs_row_major_ordering.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688391b7-5a51-4130-8b07-c0e99ca88694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00194227-3884-4959-88ad-9bd4c30de675",
   "metadata": {},
   "source": [
    "- **Is it more efficient than our solution with a 2D grid? Why?** <font size=\"5\">🤔</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868c65f-0d44-4a7c-96f0-cf24795bf867",
   "metadata": {},
   "source": [
    "## Testing Correctness\n",
    "\n",
    "Let's verify our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d65f3a-3d6f-4c0c-bc38-cc2849f2c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_correctness(M=512, N=512, K=512, atol=1e-4, rtol=1e-4):\n",
    "    \"\"\"Test if Triton implementation matches PyTorch.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    a = torch.randn(M, K, device=DEVICE, dtype=torch.float32)\n",
    "    b = torch.randn(K, N, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    # Compute with PyTorch\n",
    "    expected = matmul_pytorch(a, b)\n",
    "    \n",
    "    # Compute with Triton\n",
    "    actual = matmul_triton(a, b)\n",
    "\n",
    "    # Compute the advanced solution with Triton\n",
    "    actual_advanced = matmul_triton_advanced(a, b)\n",
    "    \n",
    "    try:\n",
    "        torch.testing.assert_close(actual, expected, atol=atol, rtol=rtol)\n",
    "        print(f\"✅ Test PASSED! Results match within tolerance.\")\n",
    "        print(f\"   Matrix dimensions: ({M}, {K}) @ ({K}, {N})\")\n",
    "        print(f\"   Max absolute difference: {(actual - expected).abs().max().item():.2e}\")\n",
    "        torch.testing.assert_close(actual_advanced, expected, atol=atol, rtol=rtol)\n",
    "        print(f\"✅ Test PASSED! Advanced results match within tolerance.\")\n",
    "        print(f\"   Matrix dimensions: ({M}, {K}) @ ({K}, {N})\")\n",
    "        print(f\"   Max absolute difference: {(actual_advanced - expected).abs().max().item():.2e}\")\n",
    "        return True\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        print(f\"❌ Test FAILED!\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run tests\n",
    "test_passed = test_correctness(M=512, N=512, K=512)\n",
    "test_passed &= test_correctness(M=37, N=42, K=73)\n",
    "test_passed &= test_correctness(M=128, N=256, K=64)\n",
    "\n",
    "# Display congrats message\n",
    "if test_passed:\n",
    "    print(\"\\n🎉 Congratulations! Your implementation is correct!\")\n",
    "    display(Image(\"figs/success.gif\", width=256, height=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f59bbd-a8de-43e7-8533-5afb8ce90f8e",
   "metadata": {},
   "source": [
    "## FLOP and Memory Analysis\n",
    "\n",
    "Matrix multiplication has much higher arithmetic intensity than our previous operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f70218-419c-44d1-99f4-3dc809d6d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_matmul_ops(M, N, K, block_m=128, block_n=128, block_k=32):\n",
    "    \"\"\"Analyze operations for matrix multiplication.\"\"\"\n",
    "    \n",
    "    # Total FLOPs: 2*M*N*K (multiply-add for each output element)\n",
    "    total_flops = 2 * M * N * K\n",
    "    \n",
    "    # Memory analysis depends on blocking strategy\n",
    "    element_size = 4  # float32\n",
    "    \n",
    "    # Naive approach: load A and B for each output element\n",
    "    naive_memory = {\n",
    "        'reads': M * N * K * 2 * element_size,  # Each output reads K elements from A and B\n",
    "        'writes': M * N * element_size,\n",
    "        'total': M * N * K * 2 * element_size + M * N * element_size\n",
    "    }\n",
    "    \n",
    "    # Blocked approach: reuse data in SRAM\n",
    "    # Each block of C requires:\n",
    "    # - Load (K/block_k) blocks of A, each of size block_m * block_k\n",
    "    # - Load (K/block_k) blocks of B, each of size block_k * block_n\n",
    "    # - Store 1 block of C, size block_m * block_n\n",
    "    \n",
    "    num_blocks = (M // block_m) * (N // block_n)\n",
    "    reads_per_block = (K // block_k) * (block_m * block_k + block_k * block_n) * element_size\n",
    "    writes_per_block = block_m * block_n * element_size\n",
    "    \n",
    "    blocked_memory = {\n",
    "        'reads': num_blocks * reads_per_block,\n",
    "        'writes': num_blocks * writes_per_block,\n",
    "        'total': num_blocks * (reads_per_block + writes_per_block)\n",
    "    }\n",
    "    \n",
    "    # Arithmetic intensity (FLOPs per byte)\n",
    "    blocked_ai = total_flops / blocked_memory['total']\n",
    "    naive_ai = total_flops / naive_memory['total']\n",
    "    \n",
    "    return {\n",
    "        'flops': total_flops,\n",
    "        'naive_memory': naive_memory,\n",
    "        'blocked_memory': blocked_memory,\n",
    "        'naive_ai': naive_ai,\n",
    "        'blocked_ai': blocked_ai,\n",
    "        'ai_improvement': blocked_ai / naive_ai\n",
    "    }\n",
    "\n",
    "# Example analysis\n",
    "M, N, K = 1024, 1024, 1024\n",
    "analysis = analyze_matmul_ops(M, N, K)\n",
    "\n",
    "print(f\"Matrix multiplication {M}x{K} @ {K}x{N}:\")\n",
    "print(f\"  Total FLOPs: {analysis['flops'] / 1e9:.2f} GFLOPs\")\n",
    "print(f\"  Naive memory: {analysis['naive_memory']['total'] / 1e9:.2f} GB\")\n",
    "print(f\"  Blocked memory: {analysis['blocked_memory']['total'] / 1e9:.2f} GB\")\n",
    "print(f\"  Naive arithmetic intensity: {analysis['naive_ai']:.2f} FLOPs/byte\")\n",
    "print(f\"  Blocked arithmetic intensity: {analysis['blocked_ai']:.2f} FLOPs/byte\")\n",
    "print(f\"  AI improvement: {analysis['ai_improvement']:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c5246-becf-46ca-9fbc-8afd47e0ff18",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've just implemented one of the most important kernels in deep learning! Here's what you learned:\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "\n",
    "1. **2D Block Tiling**: Maximizing data reuse by processing blocks\n",
    "2. **Arithmetic Intensity**: Matmul achieves a much higher arithmic intensity than vector add\n",
    "3. **Auto-tuning**: Automatically finding optimal configurations\n",
    "4. **Software Pipelining**: Using `num_stages` to overlap compute and memory\n",
    "5. **Accumulation**: Keeping partial results in SRAM\n",
    "\n",
    "### Performance Insights:\n",
    "\n",
    "- **Data Reuse**: Each element loaded participates in many operations\n",
    "- **SRAM Utilization**: Accumulator stays in fast memory throughout K loop\n",
    "- **Parallelism**: 2D grid exploits massive GPU parallelism\n",
    "- **Auto-tuning**: Different sizes need different configurations\n",
    "\n",
    "### Advanced Tips:\n",
    "\n",
    "- Experiment with different `GROUP_SIZE_M` values for L2 cache optimization\n",
    "- Try mixed precision (FP16 with FP32 accumulation) for even higher performance\n",
    "- Consider Split-K parallelization for very large K dimensions\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Ready for LayerNorm? The next puzzle introduces:\n",
    "- Parallel reduction patterns\n",
    "- Online algorithms (single-pass computation)\n",
    "- Warp-level primitives\n",
    "- Handling of statistics (mean, variance)\n",
    "\n",
    "<img src=\"figs/sardine-challenge.png\" width=\"512\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30061740-a402-46b7-82a6-10699876df90",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmarking (GPU only)\n",
    "\n",
    "Now let's benchmark the implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fb357-a70b-4e80-afc9-6604388e749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['size'],  # Square matrix size\n",
    "        x_vals=[128 * i for i in range(2, 33)],\n",
    "        line_arg='provider',\n",
    "        line_vals=['pytorch', 'triton', 'triton_advanced'],\n",
    "        line_names=['PyTorch', 'Triton', 'Advanced'],\n",
    "        styles=[('green', '-'), ('red', '--'), ('orange', '-.')],\n",
    "        ylabel='TFLOPS',\n",
    "        plot_name='matmul-performance',\n",
    "        args={},\n",
    "    )\n",
    ")\n",
    "def benchmark(size, provider):\n",
    "    \"\"\"Benchmark matrix multiplication.\"\"\"\n",
    "    M = N = K = size\n",
    "    a = torch.randn(M, K, device=DEVICE, dtype=torch.float32)  # try changing to float16\n",
    "    b = torch.randn(K, N, device=DEVICE, dtype=torch.float32)  # try changing to float16\n",
    "    \n",
    "    quantiles = [0.5, 0.05, 0.95]\n",
    "    \n",
    "    if provider == 'pytorch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: matmul_pytorch(a, b), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: matmul_triton(a, b), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'triton_advanced':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: matmul_triton_advanced(a, b), quantiles=quantiles\n",
    "        )\n",
    "    \n",
    "    # Calculate TFLOPS\n",
    "    flops = 2 * M * N * K\n",
    "    tflops = lambda ms: flops * 1e-12 / (ms * 1e-3)\n",
    "    \n",
    "    return tflops(ms), tflops(max_ms), tflops(min_ms)\n",
    "\n",
    "print(\"Running benchmarks...\")\n",
    "results = benchmark.run(show_plots=True, print_data=True, return_df=True, save_path='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367e72d-b3e9-445d-8006-1bf0d8b6573a",
   "metadata": {},
   "source": [
    "## Speedup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c6391-cb20-48cc-81e1-9edae56774b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Triton is faster than PyTorch\n",
    "avg_pytorch = results['PyTorch'].mean()\n",
    "avg_triton = results['Triton'].mean()\n",
    "speedup = avg_triton / avg_pytorch\n",
    "\n",
    "if speedup > 1.0:\n",
    "    print(f\"\\n🚀 Awesome! Triton is {speedup:.2f}x faster than PyTorch!\")\n",
    "    display(Image(\"figs/gpu.gif\", width=400, height=256))\n",
    "else:\n",
    "    print(f\"\\n🐌 Not quite yet! Triton implementation is {speedup:.2f}x slower than PyTorch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc57663-328c-420c-88e6-9349fb4ab173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
