{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d46941-1207-4662-b6ef-8c3d4ccf59a4",
   "metadata": {},
   "source": [
    "# From PyTorch to Triton\n",
    "\n",
    "Torch docs: https://docs.pytorch.org/docs/stable/index.html\n",
    "\n",
    "Triton docs: https://triton-lang.org/main/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ef548-5c18-4a2f-9071-a11780c8b68f",
   "metadata": {},
   "source": [
    "## Why write Triton kernels at all?\n",
    "\n",
    "| Scenario | PyTorch eager | `torch.compile` | Triton |\n",
    "|-----------|--------------|-----------------|--------|\n",
    "| Simple ops, plenty of kernels exist | âœ… | âœ… | âŒ (overkill) |\n",
    "| Chain of ops â†’ kernel fusion needed | âš ï¸ limited | âœ… sometimes | â­ **full control** |\n",
    "| Novel math / memory pattern | âŒ | âŒ | â­ **write it yourself** |\n",
    "\n",
    "*In short:* Triton is for the last two rowsâ€”when you need **peak GPU throughput** and/or **custom data movement** that frameworks canâ€™t fuse for you.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9e330-593e-4d11-848a-c52e1d2f983c",
   "metadata": {},
   "source": [
    "## Strides, contiguity, and why they matter\n",
    "\n",
    "A 3Ã—4 tensor laid row-major (C-contiguous):\n",
    "\n",
    "- `A.data` -> a00 a01 a02 a03 a10 a11 a12 a13 a20 a21 a22 a23\n",
    "- `A.stride(0)` -> 4\n",
    "- `A.stride(1)` -> 1\n",
    "\n",
    "*Per-dim stride* = *#elements to skip* to move by 1 in that dim.  \n",
    "Contiguous tensors have monotonically decreasing strides; views (e.g., transpose) donâ€™t.\n",
    "\n",
    "\n",
    "For example, `A.stride(0) = 4` means I need to walk `4` cols in order to arrive at the next row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94a685d-22c3-41b3-b093-6db8364301a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strides: (4, 1)\n",
      "Contiguous: True\n",
      "Strides: (1, 4)\n",
      "Contiguous: False\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Hands-on with strides\n",
    "B = torch.arange(12, dtype=torch.float32, device='cuda').reshape(3, 4)\n",
    "print(\"Strides:\", B.stride())        # (4, 1)\n",
    "print(\"Contiguous:\", B.is_contiguous())  # True\n",
    "\n",
    "C = B.t()           # transpose: shape 4Ã—3  -> it's doing a \"view\" of B. That is, changing the stride!\n",
    "print(\"Strides:\", C.stride())        # (1, 4)\n",
    "print(\"Contiguous:\", C.is_contiguous())  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c406e7a-5137-4e3e-9a0a-88df6f88262e",
   "metadata": {},
   "source": [
    "ðŸš¨ **Kernel warning**: \n",
    "\n",
    "Triton gets raw pointers plus strides -- You must be aware of them.  \n",
    "\n",
    "If you *require* contiguous, call `.contiguous()` in PyTorch **before** launching the kernel. \n",
    "\n",
    "To the best of my knowledge, all kernels I've seen assume contiguous memory for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8d37a-7290-4ffa-ab99-be7411591773",
   "metadata": {},
   "source": [
    "## Triton: how it sees your tensors\n",
    "\n",
    "Triton *does not* define its own tensor class; you pass **plain `torch.Tensor`s**:\n",
    "\n",
    "```python\n",
    "matmul_kernel[grid](\n",
    "    A, B, C,                     # tensors (device = CUDA)\n",
    "    M, N, K,                     # scalars (ints)\n",
    "    A.stride(0), A.stride(1),    # .stride(i) returns the ith stride value (int)\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "Inside a Triton kernel you receive only pointers to your tensors along with integer strides.\n",
    "\n",
    "Everything else (shapes, dtype, device) must be tracked via arguments you pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd7be8-8c8c-4a4f-be59-c9aa2c63a213",
   "metadata": {},
   "source": [
    "## Hello World in Triton\n",
    "\n",
    "Below cell is **fully working**; run it to check your setup.\n",
    "\n",
    "Don't worry about it just yet. We will go over each line, step-by-step, in the next notebooks.\n",
    "\n",
    "<img src=\"offsets.png\" width=\"512\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5e5255d-579f-4792-ba90-a50c6669e596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit  # compile-time decoration (this is what makes a kernel)\n",
    "def square_kernel(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    idxs = pid * BLOCK_SIZE + offsets\n",
    "    x = tl.load(x_ptr + idxs)\n",
    "    tl.store(out_ptr + idxs, x * x)\n",
    "\n",
    "# Launch\n",
    "N = 128\n",
    "BLOCK_SIZE = 16\n",
    "x = torch.randn(N, device='cuda', dtype=torch.float16).contiguous()\n",
    "out = torch.empty_like(x)\n",
    "grid = (triton.cdiv(N, BLOCK_SIZE),)\n",
    "square_kernel[grid](x, out, N, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "# Compare with groundtruth\n",
    "torch.allclose(out, x**2, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786d98a-c7f1-40d1-903b-4b84112ffe0b",
   "metadata": {},
   "source": [
    "## Whatâ€™s next?\n",
    "\n",
    "* Puzzle 1 â€“ Vector Addition  \n",
    "* Puzzle 2 â€“ Fused Softmax  \n",
    "* Puzzle 3 â€“ Matmul (GEMM)\n",
    "* Puzzle 4 - LayerNorm\n",
    "* Puzzle 5 - Cross-Entropy\n",
    "* Puzzle 6 - Softmax Attention\n",
    "* Puzzle 7 - Sparsemax Attention\n",
    "\n",
    "Happy hacking!  \n",
    "\n",
    "<img src=\"sardine-evolution.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be927b-fb69-44f1-9cef-02a32c4dab34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
