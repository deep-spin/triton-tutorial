{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e418f1-72fb-44eb-942b-1b4f9a866fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_triton import setup_triton\n",
    "setup_triton()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d46941-1207-4662-b6ef-8c3d4ccf59a4",
   "metadata": {},
   "source": [
    "# From PyTorch to Triton\n",
    "\n",
    "Torch docs: https://docs.pytorch.org/docs/stable/index.html\n",
    "\n",
    "Triton docs: https://triton-lang.org/main/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ef548-5c18-4a2f-9071-a11780c8b68f",
   "metadata": {},
   "source": [
    "## Why write Triton kernels at all?\n",
    "\n",
    "| Scenario | PyTorch eager | `torch.compile` | Triton |\n",
    "|-----------|--------------|-----------------|--------|\n",
    "| Simple ops, plenty of kernels exist | ✅ | ✅ | ❌ (overkill) |\n",
    "| Chain of ops → kernel fusion needed | ⚠️ limited | ✅ sometimes | ⭐ **full control** |\n",
    "| Novel math / memory pattern | ❌ | ❌ | ⭐ **write it yourself** |\n",
    "\n",
    "*In short:* Triton is for when you need **peak GPU throughput** and/or **custom data movement** that frameworks can’t fuse for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9e330-593e-4d11-848a-c52e1d2f983c",
   "metadata": {},
   "source": [
    "## Strides, contiguity, and why they matter\n",
    "\n",
    "A 3×4 tensor laid row-major (C-contiguous):\n",
    "\n",
    "- `A.data` -> a00 a01 a02 a03 a10 a11 a12 a13 a20 a21 a22 a23\n",
    "- `A.stride(0)` -> 4\n",
    "- `A.stride(1)` -> 1\n",
    "\n",
    "*Per-dim stride* = *#elements to skip* to move by 1 in that dim.  \n",
    "Contiguous tensors have monotonically decreasing strides; views (e.g., transpose) don’t.\n",
    "\n",
    "\n",
    "For example, `A.stride(0) = 4` means I need to walk `4` cols in order to arrive at the next row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a685d-22c3-41b3-b093-6db8364301a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 3.1 Hands-on with strides\n",
    "B = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "print(\"Strides:\", B.stride())        # (4, 1)\n",
    "print(\"Contiguous:\", B.is_contiguous())  # True\n",
    "\n",
    "C = B.t()           # transpose: shape 4×3  -> it's doing a \"view\" of B. That is, changing the stride!\n",
    "print(\"Strides:\", C.stride())        # (1, 4)\n",
    "print(\"Contiguous:\", C.is_contiguous())  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c406e7a-5137-4e3e-9a0a-88df6f88262e",
   "metadata": {},
   "source": [
    "## Kernel writing\n",
    "\n",
    "Triton gets **raw pointers** plus **strides**. Therefore, you must be aware of the strides!  Use `.contiguous()` in PyTorch before launching the kernel whenever necessary. \n",
    "\n",
    "> To the best of my knowledge, all kernels I've seen assume contiguous memory for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8d37a-7290-4ffa-ab99-be7411591773",
   "metadata": {},
   "source": [
    "### How Triton sees your tensors\n",
    "\n",
    "Triton *does not* define its own tensor class; you pass **plain `torch.Tensor`s**:\n",
    "\n",
    "```python\n",
    "matmul_kernel[grid](\n",
    "    A, B, C,                     # tensors (device = CUDA)\n",
    "    M, N, K,                     # scalars (ints)\n",
    "    A.stride(0), A.stride(1),    # .stride(i) returns the ith stride value (int)\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "Inside a Triton kernel you receive only pointers to your tensors along with integer strides. Everything else (shapes, dtype, device) must be tracked via arguments you pass. Let's see an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd7be8-8c8c-4a4f-be59-c9aa2c63a213",
   "metadata": {},
   "source": [
    "## Hello World in Triton\n",
    "\n",
    "The cell below is **fully working**; run it to check your setup.\n",
    "\n",
    "Don't worry about it just yet. We will go over each line, step-by-step, in the next notebooks.\n",
    "\n",
    "<img src=\"figs/offsets.png\" width=\"640\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5255d-579f-4792-ba90-a50c6669e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit  # compile-time decoration (this is what makes a kernel)\n",
    "def square_kernel(\n",
    "    x_ptr, \n",
    "    out_ptr,\n",
    "    n_elements,  # this can vary\n",
    "    BLOCK_SIZE: tl.constexpr  # this is a constant to Triton\n",
    "):\n",
    "    # recover the program id for the first grid axis \n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    # array containing [0, 1, ..., BLOCK_SIZE]\n",
    "    offsets = tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # define the exact indices for a given pid (see image above)\n",
    "    idxs = pid * BLOCK_SIZE + offsets\n",
    "\n",
    "    # load the content using these indices from HBM into SRAM\n",
    "    x = tl.load(x_ptr + idxs)\n",
    "\n",
    "    # perform computation in SRAM\n",
    "    x_squared = x * x\n",
    "\n",
    "    # save x_sq into HBM\n",
    "    tl.store(out_ptr + idxs, x_squared)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# number of elements\n",
    "N = 12\n",
    "\n",
    "# size of each chunk\n",
    "BLOCK_SIZE = 4\n",
    "\n",
    "# my data\n",
    "x = torch.randn(N, dtype=torch.float16).contiguous()\n",
    "\n",
    "# allocate output memory\n",
    "out = torch.empty_like(x)\n",
    "\n",
    "# run \"num_blocks threads\" in parallel\n",
    "grid = (triton.cdiv(N, BLOCK_SIZE),)\n",
    "\n",
    "# launch the kernel by passing the grid as a decorator argument\n",
    "square_kernel[grid](x, out, N, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "# after this line, the output is stored in the `out`\n",
    "# which is the pointer we used in `tl.store`\n",
    "\n",
    "# Compare with groundtruth\n",
    "torch.allclose(out, x**2, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786d98a-c7f1-40d1-903b-4b84112ffe0b",
   "metadata": {},
   "source": [
    "## What’s next?\n",
    "\n",
    "* Puzzle 1 – Vector Addition  \n",
    "* Puzzle 2 – Fused Softmax  \n",
    "* Puzzle 3 – Matmul (GEMM)\n",
    "* Puzzle 4 - LayerNorm\n",
    "* Puzzle 5 - Cross-Entropy\n",
    "* Puzzle 6 - Softmax Attention\n",
    "* Puzzle 7 - Sparsemax Attention\n",
    "\n",
    "Happy hacking!  \n",
    "\n",
    "<img src=\"figs/sardine-evolution.png\" width=\"512\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be927b-fb69-44f1-9cef-02a32c4dab34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
